{"pages":[{"title":"About Me","text":"Contact me: contact.wolf.site@gmail.com Hi and welcome to my blog! My name is Wolfgang, and I'm a tech worker in Texas. I have a B.S. with specializations in Production Management and German studies. I am interested in data science and all the tools that help me analyze the world. Python is my choice language. Topics My goal here is to be informational. Any content that is shallow or already covered well elsewhere will not be included. However I'm also writing up reference material I'd like to track. Hardware section is self-explanatory. Projects section contains software and data projects. Personal is a miscellaneous catch-all. Software section for pure software articles. About Blog This blog was generated using Pelican, a python language static site generator, and the theme is based off of Kevin Deldycke's 'Plumage' theme. Let me know if at any time your experience on this blog is slow or anything unsecure appears. You can view my website scripts on my github here . I do not run any ads or collect user metrics. I have added a static comment section, feel free to post a comment on any article using Markdown syntax if you would like. This requires your browser to have a Mailto setup as the comment is in fact emailed to me for approval. I like this system because it allows you to remain anonymous, use advanced syntax and the interaction is entirely between us with no 3rd party software. How flexible is Markdown? I use Markdown to format my posts.","tags":"pages","url":"about-me/"},{"title":"Learning GIS with QGIS - HPD recent crime","text":"I've had fun with Tableau recently, but I've had to trust Tableau's data processing and sometimes settle for what the software can do and not what the data can do. One particular area is working with geographic data. So I've decided to learn software for Geographic Information Systems. Perhaps the most notable GIS software is ArcGIS, but this is paid software so I continued my pattern of exploring open software and have decided to use QGIS . I chose QGIS for its large established community. Alright, I've got a long ways to go, but I wanted to share a quick visual examination of some GIS data to show how quick and powerful geographic data can be. The Houston Police Department makes available recent crime data so I've grabbed the latest issue to look at. HPD Recent Crime Here is the data as you first see it. Just a collection of points with associated reports. You can see some curiousities already. Open \"crime free\" areas within the city and some geographical shape to the border of the city. Now I'll start adding layers to better understand these patterns. Here I add Zillow's neighborhood data which includes shape files that trace out residential neighborhoods. This does an alright job of orientating someone who is familiar with Houston. Now you can see the Buffalo Bayou boundary, the Bellaire cutout, even the neighborhood shaping by 610. However there was something I was noticing when I zoomed in that I needed one more layer to tease this answer out from the data. Here I use TxDOT's road map of Texas as another layer. With GIS software, layering multiple map layers is mostly automatic and highly accurate. Now you can see the effect of roads and freeways. You can see the I-45 to Galveston is a strong area of crime and downtown has a heavy general level of crime. This final zoom section focuses on SW Houston where you can see what I consider to be the most interesting part of the data visually. (Also personally.) Here we see multiple horizontal bands of crime. While I'd expect to see that around I-10, I also see it around Bissonnet-S.Braeswood but especially all along Westheimer. Even higher prevalence of crime than I-10! Based on just this shallow exploration with GIS, I'd consider Westheimer to be a \"highway to crime\" considering it is only second to the I-45 corridor in directional crime prevalence. Note: These dots are often overlapped address data, I found many addresses which had 7+ recent crimes reported. (Last month or two.) There's so much more to learn from this data so I'll continue. I hope you enjoyed this look at GIS data and you avoid Westheimer late at night. (Just last week a friend of mine was robbed on Westheimer.)","tags":"Projects","url":"learning-gis-with-qgis-hpd-recent-crime.html"},{"title":"Grand Tour - Budapest","text":"In the backpacker community there are legendary cities. Amsterdam, Prague, Berlin, Budapest. When I was growing up I thought of Budapest as being in the same mythical category as Timbuktu, Kathmandu. Cities I read about but wasn't even sure still existed or had ever existed. Well today I can confirm Budapest does in fact exist, but it's every bit as foreign as I'd imagined. Arriving in Budapest was like many of my other budget arrivals, save money and time by traveling at night. You don't waste a day traveling and don't have to stress over checking out and rushing to transport, or arriving late and hoping you can still check in. Instead I went to sleep in Krakow and woke up in Budapest. I haven't mentioned my favorite mode of travel, but it was the network of bus companies that crisscross the EU. The prices are extremely competitive and every one I used had wifi, restrooms and stops on the way (but not too many.) I'm less picky than most when it comes to sleeping positions, I can sleep bent in half with my head on my knees in a seated position, comfortably. Unfolded and warmed up, I stepped out into Budapest in the dark early morning, outside of town. Tried to catch a tram, but it looks like instead I caught a tram driver calling it quits. I really couldn't understand the situation, my first experience with Hungarian, but it seemed the driver wasn't up for it and he just turned the lights out and kicked off everyone who had been waiting for him to take off... Well. So I discovered Budapest has a large subway system! Then I was off into the city. Two bonuses of arriving early, like a few other arrivals I managed to discover the city just as the sun was rising, then I took my time choosing where I wanted to stay that night. Now I circle back to this list of \"legendary\" cities. Budapest was unique for me in how isolated and self-sufficient the hostels are. They cater to a crowd of young people there for a weekend or an alternative lifestyle. It's perfectly normal to never speak with a Hungarian your entire stay in the city. Even the staff were entirely foreign and mostly Australian. In fact Australians and New Zealanders made up a chief portion of the travelers I encountered, or perhaps they just seemed that way due to their outgoing nature. But what about Hungary? Unfortunately I only experienced Budapest. Nevertheless it is their crown jewel, and they have a beautiful city. The river cuts through the middle separating Buda from Pest, and it is lined with gleaming commercial buildings and monumental buildings of governmental and historical significance. Nighttime river cruise The rest of the city is grand with a Parisian feel. Like other European capitols it has a massive subway system, an underground city that shields its citizens from winter and probable nuclear attack. In the statues and museums they trace a history unlike any other European people. The seven tribes who came searching for a new land to settle found it at the edge of a river where the plains ended and the hills began. Due to their distant origin they have a unique and intimidating language, their genealogy is unlike other European tribes and they have a proud independence. The story of Soviet occupation was one I'd started to experience in Warsaw, but in Hungary it was heavy. One out of four Hungarian families had a member of their family imprisoned, tortured or killed by the Soviet occupiers. The government had allied with the Nazis and been harsh, but the Soviets who replaced them took the signs down on their torture chambers and put their own immediately up. Then they ruled for 50 years. Hostel life was a lot to handle, and it was the most intensely social place I visited. Budapest has a network of Ruin Bars, which are typically formerly industrial sites in the city that have been remodeled into hip bars without much care for the remodeling. The hostels do a vigorous job of running you like a racehorse from one to the other. But I survived, enough that I had done enough wandering of the city and planned my next destination. From Budapest, to Vienna. Vajdahunyad Castle is a palace, a castle, a monument to Bela Lugosi, it's right in the city and an excellent representation of Hungarian irreverence and love of life","tags":"Personal","url":"grand-tour-budapest.html"},{"title":"Studying Houston's air transportation - in Tableau","text":"Here's another look at Bureau of Transportation data, this time examining all 2016 flights originating and/or departing a U.S. city. I've narrowed down the focus to just examine Houston flights and determine the most important connections to other cities in terms of passenger and freight data. And just for fun the first sheet is for if you want to get far away, which Houston airline will take you furthest? Source: https://public.tableau.com/views/AirTravel/HoustonOnward?:embed=y&:display_count=yes Summary The highest proportion of flights departing Houston are going to land in Texas, but the next two options are California and Florida. The top destination city is Los Angeles by total number of passengers, but Atlanta receives the most flights. The top international destination is Mexico City, with Cancun and London following. Air freight volume departing Houston lands in Memphis, Anchorage and Louisville in that order. The data also suggests that FEDEX has a major hub in Memphis and UPS has their hub in Louisville.","tags":"Projects","url":"studying-houstons-air-transportation-in-tableau.html"},{"title":"Book Review: Oil and Gas Production in Nontechnical Language","text":"I wanted to review this book or at least provide a quick recommendation of it for anyone wanting an overview on Oil & Gas. Written by Martin Raymond and William Leffler, Oil and Gas Production covers the history and current practices of petroleum exploration and production. As a general overview this book succeeds in sharing geology concepts relevant to field identification, plate tectonics, sediments and porosity, the theories of petroleum origins (Biogenic, Abiogenic) and the various classifications for gas and crude. Surveying technology, drilling technology and well stimulation are all covered as well. Fracking is of course a hot technology, improvement of which has played a large role in pushing the U.S. to the top of petroleum extraction. This book helps explain this is an umbrella term and the broader term well stimulation can be mechanical, chemical or other methods and these are always being innovated. So far you may have picked up that this book is more so for an overview of upstream production and doesn't cover downstream refining or transportation. However if any of these topics mentioned are interesting or even sound a little mysterious, this should be a quality read for you.","tags":"Personal","url":"book-review-oil-and-gas-production-in-nontechnical-language.html"},{"title":"Grand Tour - Poland","text":"I landed at Warsaw International and made it to my hostel in Nowy Świat. Even though I hadn't learned Greek, I had grown used to it and now once again everything was unfamiliar. The first impression of Warsaw was cleanliness and massive scale. Compared to Greece, everyone seemed... busier. Ultimately I spent a few weeks in Warsaw, the weather was gloomier, the internet was faster, the business atmosphere was more conducive to studying I'd been looking forward to. In the hostel community I met some great people again, on my first night I was drafted into a bachelor party visiting from Łódź. A long crazy night but a warm welcome to Poland. Next morning's breakfast was warm vodka followed by warm beer, eventually followed by a Talerz Kebab at a Kebab King. I moved to a quieter part of town eventually. I also relocated to Gdańsk, it's a smaller city on the Baltic coast. Fairly famous for its role in recent history, Gdańsk was the impetus for Hitler's invasion of Poland and the beginnings of the Solidarity movement that helped bring down Soviet rule. The most noticeable quality is the Hanseatic style of the old town, looking more like Copenhagen than Warsaw. Both cities it's also important to note were meticulously rebuilt by the Polish government and have impressive central old towns mirrored to their pre-WW2 appearance. From Gdańsk I also visited other nearby coastal cities like Sopot and even visited Falowiec, the longest continual apartment block in Europe. In the area as well as elsewhere in Poland, massive public housing projects sprawl out and up. Although the Soviets were responsible for these brutalist monoliths, I admire the pragmatic designs. Regimented apartment blocks broken up by parks and schools with nearby public transport. Although the Soviets weren't the future, some of these public planning principles will have to be. Aerial image of Falowiec in Gdańsk which houses 6,000 residents. So far I haven't mentioned the food but it exceeded expectations, even the milk bars which were a cheaper option to try Polish food. The currency exchange rate is very generous so nicer restaurants outside Warsaw were doable. I eventually wound up in Krakow and there I started enjoying pork knuckle (wieprzowina golonka), an extremely tender meat that I'd rate up there with great brisket. This isn't a food blog so I'll just say I didn't encounter anything I didn't like. Krakow was a great city for me. Not only were the historical districts mostly untouched by war, but the social streets and squares reminded me of Greece. Being a university city there was a big youth presence as well as tourists coming for the history, including Wawel Castle. I could see why so many people I met consider it to be their favorite Polish city. I'm skimming over a lot but then again I spent a long time in Poland. After Krakow I went to Zakopane, another favorite of many. It's a small resort town to the south in the Tatras mountains. Here I stayed at a cabin with more travelers, many working there as a costfree way to enjoy a ski lodge. I was able to climb more mountains and get lost some more, and of course enjoy more great food. I zigzagged across Poland, met so many more people, plane train and automobile and unlimited pierogis, but everything comes to an end. Thankfully this ending was me leaving for Budapest. From the northern coast of Gdańsk to the southern Tatras mountains.","tags":"Personal","url":"grand-tour-poland.html"},{"title":"Tableau study of top 150 U.S. ports and shipping tonnage","text":"This is a study of the top 150 ports for the U.S. and territories, data comes from the Bureau of Transportation Statistics. Note all specific values are for shipping tonnage and shipping value is not discussed in this study. This is about the scale of raw material moving in and out of U.S. ports. The longterm goal is to better understand U.S. commercial infrastructure and grow more comfortable with common methods of procuring and analyzing this data. Tableau This is the first study I'm putting up with Tableau, while things look nice and I like how interactive the visualizations are I'm still worried I don't have control over hosting the data. This is a \"story\" with four slides that can be clicked through. The main points I'm hoping to understand are the shipping volumes by state, by port, and the corresponding export/import volumes. Port Tonnage Study Source: https://public.tableau.com/shared/6MQ9NB3B8?:display_count=yes I can't imbed this visualization story right now, so here is the first sheet but please click on this link to view the entire Tableau story. (Four sheets total.) Some takeaways are the massive shipping structures of Texas and Louisiana, likely Oil & Gas related materials. Washington has a large exporting infrastructure comparatively. California has the largest importing structure comparatively. When broken down by port, Houston holds the title of largest importer and exporter by tonnage. Impressive!","tags":"Projects","url":"tableau-study-of-top-150-us-ports-and-shipping-tonnage.html"},{"title":"Grand Tour - Attica and Crete","text":"Having landed in Greece, I rode the bus into Athens and caught a taxi the rest of the way. Just like that I had made it to Europe. And in the historical sense I had made it to the beginnings of Europe. I ended up deeply enjoying Greece, but Athens never grew on me. Greece was still in the throes of the migration crisis and the streets felt tense. Many stores were shuttered and anarchists vandalized every block. Even the entire central area of Exarchia is supposedly off limits to people who don't fit in. I didn't entirely buy that, but I also didn't test it. What Athens does well is historical sites for sure, it's their bread and butter. The Acropolis Museum is new and has a premiere location next to the Acropolis, but I preferred the more worn looking National Archaeological Museum. I didn't realize when walking in that I'd be coming across the death mask of Agamemnon, the Antikythera mechanism and an enormous ancient Egypt exhibit. I also noticed the museum is overflowing with statues, so much so they have to rotate them out of storage, but I doubt they rotate out their famous Artemision Bronze . Traveling halfway across the world just to look at museums? Nope! I also visited some islands and spent a week on Crete. I did beach stuff, I hiked the Samaria Gorge and dodged gangs of goats. Slept on the decks of overnight ferries, caroused with rustic Quebecois and discovered the magic of Raki. It was here on Crete that I really got plugged into the backpacker community and started figuring out plans for the rest of my trip. The next stop after Crete was a return to Athens, this time feeling a little wiser. Then I took a train into mainland Greece to see the monasteries of Meteora. I stayed in Trikala at a hostel where I was given a hand drawn map of the hiking routes to the monasteries. It was a masterful map and saved my butt. Of course before that day's hike to Kalabaka I went to a market and was pressured into buying several pounds of grapes. Not as refreshing after carrying them for several hours. It was a small effort compared to the industriousness of the monks who erected these mountaintop monasteries and frescoed the ceilings and walls with intricate orthodox art. And for that their skulls were stacked to the ceiling in one of the rooms I saw... I took a photo before I felt rather guilty. Here as in the rest of Greece I enjoyed Gyros every day! The typical price was €1.60 for a chicken souvlaki, which came on a pita with potatoes, tomatoes and lettuce with some yellow mayo I didn't much care for... I asked for Tzatziki when I could. However a sit down meal was a different affair. A dinner was a feast with the customary Greek salad, the Greek beers, some Ouzo at the end and throughout all kinds of dishes. They also have a higher standard for feta cheese that pays off. After saying my goodbyes to new friends I was off again, running across the town (I do mean across the entirety of Trikala) to catch the train back to Athens to leave soon after on a flight to Poland. p.s. the strongest recommendation of Greece I can give is that I stayed there over a month and never felt bored. I did something new every day, but I would've been just as satisfied doing the same. It was a peaceful lifestyle, the nearby beach, the dinners with friends and family at midnight, the local squares crowded with all walks of life. Just dodge the scooters, even on the sidewalk, and you'll enjoy Greece too.","tags":"Personal","url":"grand-tour-attica-and-crete.html"},{"title":"Traveling the Grand Tour - New York launch","text":"Last year I spent a couple months wandering Europe. I've avoided talking about it here because I don't think it's technically relevant, but recently I've seen some people list it in their general development so I thought I'd do the same. It also helps answer the question, \"So what have you been up to?\" Another question could be, why? I always wanted to obviously, but I also said I would. Since 9th grade in high school when I began studying German, I wanted to achieve fluency and see Germany for myself. Since then I've also followed European events and wanted to develop for myself a picture of things there. In other words I was fulfilling the Gen-ba of seeing the real situation with my own eyes. Another reason of course is I couldn't afford to go until I finished my schooling and worked for a while. So not only was it a personal achievement for me, but it was the beginning of me seizing my career and developing the skills to get to where I want to go. Here's the beginning of my trip. I started my trip flying into New York, it was my first time there and I got a lot out of it. Central Park really is massive. The Metropolitan Museum is rich with ancient history and artifacts. Something that struck me was the prevalence of Syncretism , the combining of religious and historical themes between disparate cultures. Such as statues of Egyptian gods wearing Roman clothing, a Buddhist statue of a snake in the tradition of the local snake worshippers. These were sometimes efforts to spread religious faiths or to allow foreign rulers to appear as heirs to traditional power. Again the frequent example is Romans appearing traditionally Roman in Italian statues, but in statues displayed in Egypt the Roman emperors are depicted in Egyptian headdresses and robes. I also had a Humans of New York moment when I wandered around Downtown NY starting around the United Nations building. As I walked up into the financial district, large chauffeured black German automobiles circling the consulates segued to sheer faces of glass and steel buildings and the financial uniform became prevalent. Suddenly my tennis shoes and tanktop stuck out. I was just looking at the hive of people swirling by when one guy stopped and told me he liked my tanktop. It's a vintage print of He-Man with muscles rippling and the caption \"Don't act like you're not impressed.\" The guy explained he grew up watching He-Man and it really took him back. Then he melted back into the uniformed crowd. That was the only humanity I saw down there. Eventually my time concluded with me running in the rain trying to find the right subway. Soaked through I finally found the right line, caught my transfer, missed my stop, caught another subway coming back, got off at the right spot and rolled up to the airport. Late. But the flight was late too. I caught my international flight with some sweet talk to the counter, \"we probably shouldn't be doing this...\" However we all made it and then I flew to Istanbul. This was my first flight across the Atlantic so naturally I didn't sleep a minute. Three movies, two meals and one landing later, I was running through Ataturk's massive airport to catch my connecting flight. Thankfully they delayed the flight to Athens but not on my behalf. So me and the dozen or so others flying from NYC->Athens hopped on the bus and made it to the waiting plane. I should be honest I didn't know what to expect when I flew into Istanbul, I chose Turkish Airlines specifically for the price discounts offered due to the unrest and coup ongoing. Most other people were fleeing the airline or any other flight plans going through Turkey, but I saw an opportunity. Flying from Texas->Greece / Germany->Texas cost me $700 total including a stop in New York. That ticket was 6 flights, 3 layovers. Not bad! Now of course I wasn't totally oblivious of the situation in Turkey, I knew the coup lasted less than two days. I had my eyes open for any signs, but really only saw enormous Turkish flags draped down the buildings that looked new. The in flight magazine however was extremely interesting. The thick magazine featured the explanations from the Turkish authorities, including Erdogan, castigating the Gullenists . Then it was followed by page after page of statements of support from western political and corporate leaders. The who's who of western power. I made it! - Istanbul And then to Greece.","tags":"Personal","url":"traveling-the-grand-tour-new-york-launch.html"},{"title":"What am I up to now? 10/2017","text":"This is an update on what I'm doing since my main focus probably won't amount to many articles. But before I get into algorithms I'll go over other things I'm currently looking into. Machine-learning Recently I was able to attend a presentation by Jeff Dean of the Google Brain project. The talk was exciting as it was a crash course in various machine learning projects carried out by Google, with strong lists of measurable benefits. From improving Google translator, breaking language down to the root idea behind phrases and not word by word translations, all the way to diagnosing diabetic retinopathy better than physicians . I took a lot of notes on their development of specialized computer chips ( TPU's ) that are designed for Tensorflow, an open source machine learning library also designed by Google Brain. But the most exciting part here isn't possibly buying a TPU, not happening, but perhaps accessing one using their cloud through which they're making 1,000 TPU's virtually available. For free! Check them out here. So I immediately started playing with tensorflow, but that was a little too forward so I've decided to stick with scikit-learn and complete some Udacity courses using it. I figure it'd be better to stick with scikit-learn and potentially switch to tensorflow once I have a grasp of machine learning in practice. Algorithms However at this point I was following up on some of my hiring research, which led to studying how to evaluate employees for technical interviews, a big and expensive problem for companies who are the most advanced when it comes to data analysis and solving complicated problems. Long story short, companies evaluate potential hires on algorithmic understanding and clarity in explanation. And it turns out I'm bad at algorithms. As far as I know. However a lot of this data science journey has been tackling things that are difficult and learning them well enough to demonstrate they've been added to my toolkit. If learning algorithms makes me a better programmer then it goes into the Skills priority queue. So far I'm pretty blown away and a little embarrassed how bad my ideas of programming were. The problem of learning superior programming looks right now to be an ever deepening Matryoshka set, but I see light at the end of the tunnel. Some resources I've come across that I'm working through: MIT's open courseware Intro to Algorithms Stanford's Coursera Algorithms The MIT one is deceptively shallow as course notes, recitation videos and practice code is available off youtube . Then of course I have a few textbooks in pdf form. Algorithms - Dasgupta The Algorithm Design Manual - Skiena Algorithms and Data Structures - Mehlhorn My strategy to learn this is simply to power through both the Stanford and MIT courses at a fast pace, then if anything isn't sinking in just go sideways into the textbooks. So far so good! Goals : Better algorithms, write my own efficient algorithms or improve those of others. Write better programs. Become a sharper scientist. Eliminate weaknesses from my toolkit.","tags":"Personal","url":"what-am-i-up-to-now-102017.html"},{"title":"Excel work samples","text":"I consider my skill level with the Office Suite to be Advanced, but while I've been making efforts to demonstrate other skills I feel I haven't done enough to show work in more standard business skills. Here I'll compile a few problems I enjoyed working on in Excel. Examples Two Product Transportation Network Here's a model for a company processing two products in its network of three plants, two warehouses and two customers. Transportation costs for each possible transfer are given as well as various constraints on demand, storage space and production rate. The optimal supply chain volumes are calculated. Plant Selection This model looks at a national company which has received customer orders at various bid prices in multiple cities. Since the company has a large production network they wish to minimize shipping costs while satisfying all customer orders. The optimal production plants are calculated to allow for the maximum profit while satisfying all customer orders. Vendor Selection Here a university is choosing computers from various vendors seeking to minimize total cost while acquiring all that they need. They have to take into account vendor constraints and prices. Here they arrive at the optimal vendor selection and order amounts. 5 Job Sequence I chose some of the nicer easier to understand samples so far, so here's one on job shop scheduling. It iterates a heuristic to calculate optimal job configuration. I hope these work samples were fun to look over, there are a lot more I left out which weren't as friendly looking. For my degree, among other tools, I used Excel to solve assignments and projects for three years.","tags":"Projects","url":"excel-work-samples.html"},{"title":"Book Review: Driving Honda - Inside the World's Most Innovative Car Company","text":"Occasionally I read interesting things so I've decided to start reviewing some of them here on my blog. The following is a review of Jeffrey Rothfeder's book Driving Honda . Why should you read this book? You want to create a culture of improvement, you don't care about convention. You're curious how a blacksmith's son who dropped out of school to work as a mechanic's apprentice, enrolled at a technical college only to be expelled for blowing off exams, who continued sneaking back into classes only to stop when he had \"learned enough\", managed to become an industrial titan who upset the automotive industry and took over the motorcycle and automotive racing world. Soichiro Honda I hinted at Honda's interesting early life, but his wandering nature and relentless enthusiasm not only lead to personal success but infused his company with competitive vigor. The book details Honda's expansion into the US, the first foreign carmaker to produce in the U.S. just as American automakers were fleeing. Honda's extensive history in racing sports, exploding in flames initially but eventually outclassing the competition. From producing small cars derided as motorcycles with four wheels to now being the world's largest producer of engines. To try and capture some of what led to this I'll describe here some Honda concepts. Honda Way The philosophy behind Honda can be described as the Honda Way, which I'll paraphrase here. individual responsibility simplicity over complexity decision making based on facts not assumptions minimalism over waste flat organization autonomous teams that are continuously accountable to one another perpetual change unyielding cynicism about what is believed to be the truth specific goals for employees and suppliers and active company participation in helping them achieve them borrow from the past to innovate the future The decision making based on facts not assumptions partially refers to sangen shugi , which means three realities. These three realities refer to Gen-ba, Gen-butsu, Gen-jitsu. The real spot, the real part, the real facts. I'll describe this as going to the site and examining the situation, understanding the situation and its specifics, then using real derived facts and observations to support your decision. This approach has caught on and now we know it as \" Gemba walks\" where a leader should inspect the situation for themselves even if a specific problem hasn't arisen yet. Making a decision without knowing the real of a situation is dangerous. Waigaya Waigaya isn't a Japanese word per se, it's simply what Soichiro used to describe the sound of babble. We can think of it as hubbub. Waigaya has now come to be defined as a system for meetings that aren't necessarily planned. If a problem arises, relevant parties should assemble to solve it then and there. It's important to know that part of Honda's flat hierarchy is a Honda uniform where every worker dresses in matching coveralls. Soichiro Honda visits his motorcycle manufacturing plant in Marysville, Ohio (1979), the first U.S. factory built by a Japanese automotive firm The principles of waigaya are as follows. Everyone is equal in Waigaya- the only bad idea is the unspoken one All ideas must be disputed until they are proven valid or vanquished Once an idea is shared, no one owns it, it belongs to everyone At conclusion, decisions and responsibilities are precisely assigned A waigaya is a discussion of an idea or concern. It can take minutes, or years. It doesn't necessarily offer a solution, but the culture it creates is one of constant questioning of current assumptions. Anyone can call a waigaya but only the group can conclude it. I'd recommend this book for all the too many things to talk about here. The many anecdotes of management culture, the history of automotive disruption, the peculiar character of Soichiro Honda. The number one reason is because continuous improvement is difficult, many companies fail or only manage to pay lip service. It's not just KPI's and powerpoints. It's people and culture. How does a start-up stay hungry like a start-up when it's spread across a globe? How does decentralization lead to innovation? How is culture generalized and preserved generationally? My Gemba walk Now for a personal touch, I bought this book in preparation for an interview with Honda Manufacturing. Prior to the interview I read it twice. The first page opens with a story of the ground breaking of the plant I interviewed at. It was hugely satisfying to see the assembly line with all its humming complexities up close. The white uniforms, the open cafeteria, the bullpen setup where everyone including the plant management had an open desk. I suppose for full disclosure I should mention I was dead in the water before I got there. Although I was interviewed by 6 people, the only personal interaction was reading of questions and furtive eye contact. The final question I won't forget, \" What do you know about Honda? \"","tags":"Personal","url":"book-review-driving-honda-inside-the-worlds-most-innovative-car-company.html"},{"title":"Meta-analysis review of predictors for job success","text":"Hiring is a complicated problem today, more complicated today than ever before and especially in states with stiffer worker regulations. Any hiring manager knows the damage bad hires can do and takes a low risk approach. But what candidate factors correlate to a successful candidate the closest? There are several published studies on this so I'll be looking at a meta analysis. Hunter and Hunter 1984 (PDF) They examined a few thousand studies and normalized the results. I'll be touching on the high points and not generating my own data analysis for this article. What I'm picking out is a small sample of the interesting conclusions and providing some analysis. The main argument of the study is how strongly Ability Testing compares to other predictors of job performance. Ability Testing is understood to be general cognitive tests. Before I show some actual numbers I should give a refresher on correlation coefficients which the numbers here represent. The range is generally [-1,1] with a 0 implying no correlation whatsoever. A 1 is associated with perfect positive correlation and -1 indicates perfect negative correlation. When studying two variables, a poor correlation of [-.2,.2] can often be termed a weak correlation. This frequently depends on the science discipline with social sciences often having to settle for accepting weaker correlations as significant. Here is my opinion on correlation coefficients. Correlation Rating 0.0 - .10 no correlation to negligible .10 - .20 minor .20 - .40 some .40 - .60 moderate .60 - .80 strong .80 - 1.0 High to perfect correlation Alright now for some tables. This table looks at Ability Testing as a predictor in performance for various job types. So we see general cognitive ability (Cog) most strongly predicts success for Salespersons, Clerks and Managers; correlations of .61, .54 and .53 respectively. The other two categories look at Perceptual and Psychomotor ability and offer lower correlations. The next table looks at specific groups of Ability Tests, pay special attention to Group 2 and Group 3. Group 2 corresponds to cognitive tests associated with entry level positions which will offer training. Group 3 corresponds to an experienced worker group. The strongest predictor remains the general cognitive ability, referred to in this table as GVN. For Group 2 it correlated at .50 and Group 3 a .57. So far we've learned ability testing that measures general cognitive ability is the strongest predictor of ability testing. Now that we know the best ability testing can offer, we can compare it to other studied predictors of job performance. This next table studies predictors for positive supervisor ratings, promotion, training success and tenure. Since it is a large table and overly detailed to my central point I'll put it in a drop drown. Detailed predictors Some insights: college GPA is important for predicting training success, but drops down dramatically for predicting promotion and positive evaluation by supervisors. Interview success has a minor correlation with success in any category. Of all predictors, peer evaluations did the best job of predicting success with correlations of .49 for supervisor ratings and promotion, but .36 for training success. This makes sense because peer evaluations have a lot to do with determining supervisor ratings and promotion. Reference checks provide some correlation across all categories. Biodata was the 2nd best predictor. What is Biodata? Biographical data about the candidate. This meta analysis reserves some criticism for this measure. Some issues noted are the tendency of Biodata to be adjusted to keep track of whichever keys did a good job of improving correlation but dropping keys which bring the correlation down. What keys are tracked depend on the study but some keys include held credentials and leadership positions held. A further observation is Biodata experiences a correlation decay in that the correlation drops dramatically over time to almost no correlation in a longitudinal study. In other words biodata that indicated a candidate would be a great hire does so accurately only on a near term basis. As time goes on the prior accomplishments of a candidate has almost no relation to their success later on. Now to put it all together. This final table lists predictors ranked by correlation for entry level jobs where training will occur after hire. We see here the results of the meta analysis looking at thousands of studies and organizations. The top predictor is Ability Testing with Job Tryout also providing moderate correlation to success. Biographical data and reference checks provide some correlation. Experience, Interviews, Training and experience ratings and Academic achievement provide minor correlation. Education and Interest provide negligible correlation. Age has no correlation. Does this mean you should give someone a general cognitive ability test and if they score high enough let them trial the job? Not necessarily. While the other indicators are weaker they probably act as a gateway to allow someone to be evaluated for what truly predicts job success. However in a pool of similarly qualified candidates perhaps with disparate backgrounds, the decider could be cognitive ability tests. Now for the wildcard strategy. If a company doesn't have a budget that can afford candidates from top schools with high GPAs or those with a strong record of leadership and accomplishments, hiring on behavioral background, high Ability Testing and on a contingent basis might get them the industry busting talent other companies overlook. Moneyball in the business world. Another idea is hiring someone of any age, with either too much or too little education and little interest for the job could still result in a successful hire. Hiring someone from a top ranked school with high achievement in academics and extra-curriculars who gives a fantastic interview and has all the perfect experience could still turn out to be unsuccessful if they lack strong references and do poorly on general ability testing. Thanks for reading.","tags":"Personal","url":"meta-analysis-review-of-predictors-for-job-success.html"},{"title":"Hurricane Harvey's Effect on Gas Price","text":"What was the economic impact of Hurricane Harvey on the price of gasoline? I can answer this personally from a Houston perspective and say it pushed my $2.09 to $2.35, and immediately after the storm it was common not only to experience prices around $3/gallon but shortages in availability. A quick note: $2/gallon equals €1.70/gallon equals €0.24/Liter. Scanning Berlin, Germany now I see €1.35/Liter there. So before I look at anything I should go ahead and check how much it's already been looked at. I found a CNBC article that claimed there wouldn't be much of a general impact but noted the RBOB gasoline futures jump of 6.8%. First thing first, what is RBOB? RBOB description linked here. Then of course CNBC's tracker is linked here. Now let's say I'm not satisfied clicking around on their website and playing with their provided tools, though they have an impressive list of built in browser tools, I want to download this dataset. I'll go back to my favorite source, Quandl. The exact thing I'm looking for EOD/UGA , but unfortunately this is listed under the Premium databases. Thankfully this only makes me go sideways. I found a free version listed under FRED/DRGASLA , downsides are it hasn't been refreshed in two days and it appears to describe gas prices in Los Angeles... But I'm just looking for historical trends so this will work. They also have a source for weekly and monthly, but I always look for the finest data (daily) and transform it if I need to. The Daily data looks pretty messy. I'd like to smooth it out, but first I'll address some of the CNBC trader's lack of concern. Here is the daily price data over the past 14 years. Since the peak as you can see to the far right of the graph was a little extreme, I found the average price for that peak month instead. This chart shows the area above that month's average and below for comparison. This shows in the past two years this price stood out somewhat, but seems more in line with the prior decade. Since prices have dropped back down after this recent peak we don't appear to be approaching the higher price levels of the past decade yet. I'd like to go a little deeper in the analysis of general trends so if interested read on. While the daily data looks messy I can go ahead and use it for a daily % change in price seen here. This data would be overwhelming if not for a loess smoother showing a trend with predicted values. A trend along the horizontal 0 line would indicate a steady fluctuation, but instead we see our trend (in red) is hovering slightly below a 0.001% daily increase in price. The trend only dives into a negative rate around 2012 and reemerges in 2016. What does this chart illustrate? Well for one it suggests someone graduating in 2014 would've had the absolute worst timing this generation to try and get into Oil & Gas, unfortunately. But also in my opinion we have discovered the normal inflationary rate of gas price and also seen the dramatic change brought on by the 2010's Oil Glut . We might expect the rate to stabilize again at 0.001%+ daily. Now I'm going to smooth things to a more manageable level visually, I'll be going with a monthly average. Note: Monthly averaging is smoother than monthly sampling Show/hide Note If I collapse monthly my Daily report using their built in function it is in fact sampling at monthly intervals. I could use their FRED/MRGASLA dataset which provides the monthly average, or if I only have access to the daily report I can calculate the monthly average in R. How big can this difference be? Here is a chart showing monthly average in blue. The red line represents the monthly sample and appears more erratic. To emphasize how strongly these individual samples can differ from the average, I have dotted the points as well. Since my goal here is to smooth my chart, I will take the monthly averages. Now that I've made a monthly chart I can plot and examine its loess curve. Well, there it is. It isn't very convincing. Other curves follow the data too well for me. Even though I've smoothed things to monthly averages, individual months still appear dramatic particularly around 2008. Do I buy the loess curve? No, but any better trend predictions will have to wait another article. Thanks for reading.","tags":"Projects","url":"hurricane-harveys-effect-on-gas-price.html"},{"title":"Reduce API calls and process time- or How I learned to pickle my quandl","text":"I've written about Quandl before and the wealth of information they provide in an easy to process way. However their generosity does know limits. While it's difficult to brush up against their daily call limits, I might as well do what I can to be respectful of their service and my efficiency. Introducing another Python package to discuss- Pickle . In short pickle allows python objects to be converted to streams and stored, then later accessed. The concept behind this is serialization. This also would allow objects to be transmitted to sockets and reproduced accurately, although pickle versioning is important. Pickle does not provide any form of security, all pickles imported are assumed to be in working order and not tampered with. Using pickle allows me to store my python dataframes and open them from their binary rather than make additional calls to quandl. The most immediate benefit is speed. This program from a previous article is now run entirely offline. Full Program here Show/hide Code #!/usr/bin/env python3 # -*- coding: utf-8 -*- #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" Effective Federal Funds Rate over time with economic recessions @author: wolf \"\"\" import pickle import quandl import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) quandl . ApiConfig . api_key = 'your-api-key-here' #-----#Uncomment following block to pull and create updated dataframes #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" , start_date = \"1955-01-01\" ) output = open ( \"EffFundsRate.pickle\" , 'wb' ) pickle . dump ( df2 , output , pickle . HIGHEST_PROTOCOL ) output . close () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) output = open ( \"FREDrecession.pickle\" , 'wb' ) pickle . dump ( df4 , output , pickle . HIGHEST_PROTOCOL ) output . close () #------#Uncomment following block to load stored dataframes pickle_in = open ( 'EffFundsRate.pickle' , 'rb' ) df2 = pickle . load ( pickle_in ) pickle_in . close () pickle_in = open ( 'FREDrecession.pickle' , 'rb' ) df4 = pickle . load ( pickle_in ) pickle_in . close () #------# df2 [ 'Value' ] . plot () maxY = df2 [ 'Value' ] . max () y2 = df4 [ 'Value' ] plt . fill_between ( df4 . index , 0 , maxY , where = y2 == 1 , facecolor = 'blue' , alpha = 0.2 ) plt . xlabel ( 'Year' ) plt . title ( 'Federal Funds effective rate with recession times shaded' ) plt . show () Note: I swapped my unique API key with a placeholder, you can run this code by deleting that line to access Quandl as an unregistered user. As an unregistered user you have lower call limits. Key points For now the source is a manual choice of commenting one block or the other. Both blocks run uncommented works but this is a complete reset, as it pulls fresh data and then overwrites the old pickle files. To run a fresh pull from quandl and write new pickle files the first block is run: #-----#Uncomment following block to pull and create updated dataframes #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" , start_date = \"1955-01-01\" ) output = open ( \"EffFundsRate.pickle\" , 'wb' ) pickle . dump ( df2 , output , pickle . HIGHEST_PROTOCOL ) output . close () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) output = open ( \"FREDrecession.pickle\" , 'wb' ) pickle . dump ( df4 , output , pickle . HIGHEST_PROTOCOL ) output . close () Once this has been performed it doesn't need to be rerun unless the source data has updated. (In this case I am pulling weekly data updated every Wednesday and collapsed by month.) After commenting out the previous block I can now uncomment the following: #------#Uncomment following block to load stored dataframes pickle_in = open ( 'EffFundsRate.pickle' , 'rb' ) df2 = pickle . load ( pickle_in ) pickle_in . close () pickle_in = open ( 'FREDrecession.pickle' , 'rb' ) df4 = pickle . load ( pickle_in ) pickle_in . close () #------# This block is now all I need to run to import my stored pickle data as a python object, as originally stored. Now I can enjoy fast 'free' data that is stored offline. Improvements: The use cases I've seen all used this commenting/uncommenting strategy, but this is a choice that would have to be made by a programmer. To make this more user friendly I could have a user prompt at the beginning when run on a command line or window asking, \"Would you like to refresh to latest source data?\" Otherwise you could be at risk of using outdated data when quandl was chosen for its up to the minute accuracy.","tags":"Software","url":"reduce-api-calls-and-process-time-or-how-i-learned-to-pickle-my-quandl.html"},{"title":"Shading a federal funds rate graph by variable","text":"There's some talk in the news about economic policy at the federal reserve taking a turn to start increasing the federal funds rate. Fed president Janet Yellen has indicated on what conditions she will raise it and it appears increasingly likely. Raising rates is meant to tamp down inflation, but shouldn't it also quiet down investment and economic growth? That's an open question and to help answer that I wanted to graph both federal funds rate and years of recession. To do this I'm using Quandl's data sets, specifically their Federal Funds Effective Rate and their Recession Indicators . Here is the code I'm using: Show/hide Code import quandl import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) quandl . ApiConfig . api_key = 'my_api_key_here' #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" ) df2 [ 'Value' ] . plot () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) maxY = df2 [ 'Value' ] . max () y2 = df4 [ 'Value' ] plt . fill_between ( df4 . index , 0 , maxY , where = y2 == 1 , facecolor = 'blue' , alpha = 0.2 ) plt . xlabel ( 'Year' ) plt . title ( 'Federal Funds effective rate with recession times shaded' ) plt . show () Note: I swapped my unique API key with a placeholder, you can run this code by deleting that line to access Quandl as an unregistered user. As an unregistered user you have low call limits. So what does my code produce? Exactly what I wanted of course, as you can see here: Full confession, this isn't the most unique connection to make, and it's one that's largely uncorrelated. Most of the fun here is just figuring out how to shade under and over a line to replicate an effect often used by Financial Times and others. The key part is making it variable based on the data itself. That'll be useful for later. Now for a real world example, here is one from the St. Louis Federal Reserve .","tags":"Software","url":"shading-a-federal-funds-rate-graph-by-variable.html"},{"title":"Weekend reading: Quandl tables, Django with Heroku","text":"I've been following some machine learning tutorials for Python by Sentdex on YouTube. He's notable for his prolific output of Python tutorials on various topics and for making the topics reachable to all audiences. He has tutorial series in financial analysis, python skills and every sort of interesting thing Python can do. Here's his machine learning series that I'm working through: Quandl Sentdex pulled sample data from Quandl using their Py module . Their website includes an API section to get you started pulling the data using whatever language or software you prefer. What is Quandl? Simply put, it's one of the coolest data providers I've come across! They mainly deal in financial data, including End Of Day stock results and fundamental indicators of company and economic health. That's just the beginning, they made a name for themselves by attracting alternative data as different as satellite measurements of global oil tank storage levels to national UFO sightings (yes the last one is real, but the jury is out on whether it's real .) The data I was most interested in was housing data from Zillow, Federal Reserve data on the economy, European Central Bank records and even China Macroeconomic and Industrial Data. It's invigorating to see 'live' data that I can easily pull in and play with and not just read summary articles by others. Will I gain insights? At this point I highly doubt it, however it's a great playground for machine learning and maybe one day I'll contribute some ideas to how things work. Heroku What is Heroku ? It's technically referred to as a PaaS (Platform as a Service), meaning they provide not just a server but a managed software environment to host your apps. They take care of scaling (based on your specifications), database management and all the background infrastructure problems which could take you down. All you do is bring your app and manage it. I'm looking at some new web service projects and Heroku stood out for its popular usage. If languages can be lower or higher level based on their complexity then PaaS could be called higher level for doing more to manage infrastructure than IaaS providers do. Infrastructure as a Service (IaaS) provides the server backbone to host your own software. PaaS providers handle software licenses and other sysadmin duties for you. Well maybe you already know this. I followed Heroku's free tutorial which featured downloading their CLI and uploading a premade Django web app. It honestly wasn't too complicated in concept and pretty familiar to my blog development process. Developing an app, applying git version control to track progress and push updates. The interesting parts were Heroku's management software, hosting your app locally, spinning up your one Free dyno to serve it from Heroku. All in all they seem to do a good job of separating the infrastructure from app development, and I wouldn't mind to work with them. That was the weekend!","tags":"Personal","url":"weekend-reading-quandl-tables-django-with-heroku.html"},{"title":"BRS Part 2","text":"This is part two of my Blended Retirement System study. For more information please see my original article, BRS Part 1 . Now that I've made a sample program which can take a user input and provide the correct pay rate, I'm going to do two things: convert my code to Python 3 (3.6) and begin creating a simulation program. The original program was written in Python 2.7 which is a legacy standard still widely used. However since I have the luxury of forward planning I will go ahead and switch to Python 3 before it becomes burdensome. I did experiment with some code conversion methods that promise to mostly automate the process, but it quickly became apparent I was just bloating my simple program. Therefore I just made the necessary changes, input and print updates. CalcPay in Python 3 Show/hide Code #!/usr/bin/env python3 \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import sys import math import pandas as pd def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print ( \"Service years was entered incorrectly. Must be >0 and <40.\" ) return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" separator = ' ' print ( \"Please enter rank followed by years in service.\" ) print ( \"Format is 'E-1' space #, ex: 'O-5 20' or 'E-3 6'\" ) user_input = input ( \": \" ) . split ( separator ) rank = user_input [ 0 ] service = user_input [ 1 ] active_input = input ( \"Active duty, Y? Reserve is N.: \" ) . lower () if active_input in ( \"yes\" , \"y\" ): active = 1 act_out = 'n active duty ' elif active_input in ( \"no\" , \"n\" ): active = 0 act_out = ' reserve ' else : sys . exit ( \"Please try again with Y or N for active duty answer.\" ) enlist = str ( user_input [ 0 ]) if ord ( enlist [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 rank = str ( rank ) service = int ( service ) years = determine_years ( service ) pay = find_pay ( rank , years , active , enlisted ) if math . isnan ( pay ): print ( \"There doesn't exist a base pay for that rank with those years of service\" ) else : print ( \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay )) if __name__ == \"__main__\" : calc_pay () Simulation Now that my basic program is converted to Python 3 I can start playing with it for the purpose of this project. As a reminder the ultimate goal is to analyze cost savings of the US military's implementation of the new Blended Retirement System. I decided to create a simulator and it is here I am running into disappointment. The goal is to create a simulator that very closely mirrors reality with accurate distributions in costs as calculated by numbers in each rank and pay rate. Here is my simulator code so far: Show/hide Code #!/usr/bin/env python3 \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import math import random import pandas as pd import numpy as np def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print ( \"Service years was entered incorrectly. Must be >0 and <40.\" ) return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" active = random . randrange ( 2 ) enlisted = random . randrange ( 2 ) rank1 = pd . read_csv ( 'usarmy2017.csv' ) rank_df = pd . DataFrame ( rank1 ) rank_list = rank_df [ 'rank' ] percent_list = rank_df [ 'percent' ] rank = np . random . choice ( rank_list , p = percent_list ) min_year = ( rank_df [ rank_list == rank ]) . iloc [ 0 , 3 ] service = random . randrange ( min_year , 41 ) years = determine_years ( service ) if ord ( rank [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 pay = find_pay ( rank , years , active , enlisted ) if active == 1 : act_out = 'n active ' else : act_out = ' reserve ' if math . isnan ( pay ): print ( \"There doesn't exist a base pay for a \" + rank + \" with \" + str ( service ) + \" years of service.\" ) else : print ( \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay )) if __name__ == \"__main__\" : calc_pay () You'll see I have eliminated user inputs, now the inputs are randomized with appropriate possible inputs. This program is still producing just one data point so far. To base my simulator in reality I am weighing the randomness by real world amounts by accessing a csv file with numpy.random.choice() . This selects a random rank based on its weighted distribution provided in the chart. You can view the chart here. Please note I have decided to narrow my development to only look at US army ranks for now. This simulator should eventually be able to work for all services and eventually return any year analysis. That's going to be a lot of csv tables! Or just redesign to operate off a database, right now I am working with PostGreSQL in other projects. Notice the first weight is provided in exponent notation which is well interpreted by NumPy. However you likely noticed the last column minyears . This column provides the commonly understood minimum number of years of service to achieve the corresponding rank. It's not a perfect science since there are performance factors involved, and it also breaks down in the upper officer ranks where they aren't clear rules. What I have created however is a lower bounds for my service years. Where now? At this point my data is starting to become imperfect and I need to pause and analyze how far I can take this simulator. I have a perfect distribution for rank. I can also create perfect distributions for questions of enlisted/officer or active/reserve. That would create a perfect simulation, but unfortunately I need to have the crucial input of service years. The only way to fix this I see is to find data of experience by rank. I've been digging through actuary.defense.gov to see their yearly reports, but haven't found something this exact yet. So I am naturally going to take this two ways at once, keep digging for real numbers, but also begin constructing a solve-for that could populate this for me using the total costs against all the known inputs I have so far. The only variable missing is a statistical distribution of service by rank. If I create this I can then achieve a most perfect simulator. However this also makes me want to work on other projects.","tags":"Projects","url":"brs-part-2.html"},{"title":"BRS Part 1","text":"Blended Retirement System costs, financial analysis of the US Military's new retirement system effective 2018 This is a multi-stage analysis of the new retirement system for all military personel going into effect January 1, 2018. Part one is a programming project in Python adding a calculator to the Department of Defense's website at militarypay.defense.gov Under the previous system retirement was only awarded after 20 years of service, either active duty years or a combination including reserve 'good years'. A good year defined as meeting sufficient reserve service requirements. The new system will be a combination of pension, bonus and investment fund matching. Since only 17% of service members stay to retire after 20 years, this new system will affect and potentially benefit the majority of service members. Calculator The calculator takes in multiple pay tables saved as csv files, one of the simplest data storage formats. These tables therefore take up minimal space and can be read and wrote to by any system. The advantage of this calculator is it allows the user to retrieve reserve or active duty pay rates. User input determines which pay table is relevant and which particular pay on the chart to return. Code The program to do this is written in Python and the code is as follows with corresponding commenting. Show/hide \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import sys import math import pandas as pd def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print \"Service years was entered incorrectly. Must be >0 and <40.\" return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" separator = ' ' #separator of input values is space print \"Please enter rank followed by years in service.\" print \"Format is 'E-1' space #, ex: O-5 20 or 'E-3 6'\" user_input = raw_input ( \": \" ) . split ( separator ) #reads in rank and service years rank = user_input [ 0 ] service = user_input [ 1 ] active_input = raw_input ( \"Active duty, Y? Reserve is N.: \" ) . lower () #reads active duty or not if active_input in ( \"yes\" , \"y\" ): #Assigns rest of program's active/reserve references active = 1 act_out = 'n active duty ' elif active_input in ( \"no\" , \"n\" ): active = 0 act_out = ' reserve ' else : sys . exit ( \"Please try again with Y or N for active duty answer.\" ) enlist = str ( user_input [ 0 ]) #This body determines whether user's rank is enlisted or officer if ord ( enlist [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 rank = str ( rank ) #Prepares user inputs for function calls with correct data types service = int ( service ) years = determine_years ( service ) #Takes user's service years and assigns experience pay range pay = find_pay ( rank , years , active , enlisted ) #Looks up correct chart and cell value for rank, years in, active or reserve, and enlisted or officer if math . isnan ( pay ): #Prints a summary statement if there exists a base pay for user input, or an error message print \"There doesn't exist a base pay for that rank with those years of service\" else : print \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay ) if __name__ == \"__main__\" : calc_pay () Screenshots Here is a snapshot of the pay table for active duty enlisted soldiers represented in raw CSV Here is a snapshot of the pay table for reserve officers automatically formatted in an excel application Here is the input prompt when the program is ran Following the first input there is a secondary input, after both are entered the ouput lists the pay rate for the selected rank, years of experience and active or reserve duty selection If an input follows appropriate formatting but doesn't choose a valid pay rate, an error note is returned Conclusion The next project in this series will be data analysis to examine system wide costs and savings.","tags":"Projects","url":"brs-part-1.html"},{"title":"Measuring program time (timeit module)","text":"I've been using python's timeit module for some testing, wanted to keep track here of timeit and any other methods I come across. Here's an example usage. import timeit start = timeit . default_timer () # stop = timeit . default_timer () print ( stop - start ) Note: default_timer uses time.perf_counter() as defined here Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration. It does include time elapsed during sleep and is system-wide. The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid. Linux Since I am using linux I can also use a built in bash command of 'time' and run the python program from the command line. $ time python program.py This will return output similar to the following. real 0m4.139s user 0m0.301s sys 0m0.053s A good explanation is found here on stackoverflow . Real is wall clock time - time from start to finish of the call. This is all elapsed time including time slices used by other processes and time the process spends blocked (for example if it is waiting for I/O to complete). User is the amount of CPU time spent in user-mode code (outside the kernel) within the process. This is only actual CPU time used in executing the process. Other processes and time the process spends blocked do not count towards this figure. Sys is the amount of CPU time spent in the kernel within the process. This means executing CPU time spent in system calls within the kernel, as opposed to library code, which is still running in user-space. Like 'user', this is only CPU time used by the process. See below for a brief description of kernel mode (also known as 'supervisor' mode) and the system call mechanism. Here is a good article about formatting options . Summary, time is a good benchmarking tool I will be using from the command line, and timeit is one I can insert into my programs and measure any segment I need. timeit Documentation here . Addendum; Here's an example usage from my program I wrote about in this blog post . I used a timeit start at the beginning of the program in the declarations, then I stopped it at the end of the main body after the final print statements. It gives me the first time value of 3.43 seconds, showing that real time measured of the main body of program was 3.43 seconds. Real time includes user prompt and my typing time! The next time values come from bash's time command, so its real time returned 3.75 seconds. Now this should be comparable to the timeit I ran from within the program, however the bash real will always be greater as it is run outside the program, so it also counts time to open and close the program as well as other boring system things like import statements. So far I haven't seen a time measure that doesn't include time I spent typing, and that's where the next two values are very useful. They come from bash's time, and they measure process time itself, see details above. So my user mode used my processor for 0.3 seconds (program management) and kernel calculations took 0.026 seconds. The kernel time is the best for comparing actual work performed. I can use this to compare to other programs.","tags":"Software","url":"measuring-program-time-timeit-module.html"},{"title":"Why overspend for smoothies?","text":"Smoothies are liquid snacks that generally taste great, and an easy way to pack in what you should be having. Now if you search Costco for smoothie mix you'll find a bag of https://www.raderfarms.com/ fresh start smoothie mix in a 3 lbs (48 oz) bag sold for around $12. Inside are 6 pouches of 8oz each. Fresh Start smoothie blend While it's a very nice mix, I've switched over to just making my own. Still shopping at Costco but now I am combining two other products sold to make a similar mix. The new mixes Earthbound Farm Power Green That's 1.5 lbs (24 oz) of Spinach, Kale and chard and Kirkland Three Berry Blend Here we've got 4 lbs (64 oz) of Blackberries, Blueberries and Raspberries Product lb oz price $/oz Rader Farm smoothie mix 3 48 12 .25 Earthbound Farm Power Greens 1.5 24 4 .17 Kirkland Three Berry Blend 4 64 7 .11 Ok just to play some more with the numbers assuming the Rader Farm smoothie mix is 50/50 greens and berries by weight, then the simple combined $/oz price for my substitute mix is .14 which is compared to the .25 of the smoothie mix. So you're paying 78% more for the prepackaged baggies of smoothie mix. Notes: You lose out on strawberries but gain blackberries. Also the smoothie blend kale is mature and the Earthbound is baby kale. Go forth and make your own smoothie mix!","tags":"Personal","url":"why-overspend-for-smoothies.html"},{"title":"Buying Guide for a budget gaming PC","text":"Goals: This is a gaming PC that can play most modern games on the maximum settings. This is also a self-build and every cost saving is used. This guide is a bit outdated since I built this a few years ago, the focus is to show how to price check and some buying recommendations as well as some installation advice. Also note this is written for a beginner level. Specifications: Part Name known as Brand Model Central Processing Unit CPU Intel i5-4690k Motherboard MOBO MSI Kraitos Graphics Processing Unit GPU AMD R9 290 4GB DCUII Solid State Drive SSD Crucial 240GB mx300 Harddrive HDD Western Digital 1TB Green internal Random Access Memory RAM 8gb 2 sticks of 4GB 1600mhz Case box NZXT S340 Accessory Brand/Model Monitor Asus VN 248 Keyboard TTesports Knucker Mouse Cobra E-3lue Headphones Logitech G230 Budget The first thing to check is your budget, a good way of getting an idea is look at some other builds and start getting an idea of the hardware you can afford in your budget. Some good sites to check are: Reddit's /r/buildapc 'Paul's Hardware' on YouTube and of course some prebuilt PC sites like Main Gear Don't be intimidated by some of the costs of $2,000+ PC builds, those tend to include dual graphics cards, custom cooling systems, the absolute best CPU and many bizarre addons like engravings, cutouts and LED lightups. No matter how aggressively styled your computer case is it won't make you click your mouse any faster. This guide assumes you're going for more of a barebones approach to a gaming PC and your home is room temperature. I also hope that the advice here is relevant to people building low end computers or one of those really high end monsters. Ok so now you have a better idea of your budget, I'm thinking under $1,000 is a good goal. As time goes on this should be more achievable. Hopefully. CPU The first thing to look at is your CPU, this determines your motherboard (MOBO). i5 or i7 processers use different socket architecture for instance, when buying a CPU look at socket (e.g. LGA1150 or LGA1151) to determine which mobo you can choose. I recommend Intel for power efficiency but not necessarily for cost and power. The trick here is to find a good deal. Keep in mind prebuilt computers won't include as many hardware deals that you can get shopping individually. This is where I start selling you on - Microcenter. Now I know locations vary and you might not even have one in your country. If Microcenter is an option be sure to include them as a key price checker. A good way to start that is stop by a store and pick up their product newspaper. There they'll list all the schemes of mixing and matching hardware to get instant discounts. Be careful when sniping off one piece at a time from Newegg or eBay as you'll miss out on deals like \"$100 off for buying a CPU and GPU together\". The meat of the matter is benchmarks. Intel and AMD create different CPU's so to learn the differences you'll have to check out benchmarks. This will be your own journey. An excellent website is cpubenchmark.net which allows you to compare CPUs as shown here. The CPU mark is a method of comparison. Keep in mind however you don't need the latest top of the line CPU, Moore's Law dictates you will increasingly be ripped off when it comes to building gaming PC's to run Hotline Miami. My i5-4690k manages to run games on max settings without having to full load and creating a necessity for cooling solutions. The stock cooler therefore works fine and that's a saving. Why doesn't my CPU get taxed by individual games? The GPU is the powerhouse that runs the games, which are chiefly graphically taxing. Don't expect too much from a lower end processer, but also don't expect your i7-7700 to need a $300 liquid cooling pump (with pulsating LEDs). Was it improper to discuss the GPU in the CPU section? Not at all. The GPU is the fundamental hardware for a gaming PC (budgetary concern), but the CPU is the fundamental hardware for a PC (technical concern). In other words your CPU determines other parts, but your GPU is independent and is also the most important part of your gaming rig. MOBO Now that you've picked your CPU you can look at corresponding motherboards. A CPU will list the socket it requires. The motherboard is the backbone for all the hardware you buy. I chose a MSI Kraitos due to the package deal offered with the CPU at Microcenter. The highlights are GPU This is the key decision point for your build, you can select from a wide range of cards and it'll be hard to tell what's right for you. The best way is to look at your budget and understand the GPU is really what pushes performance. In other words this will be your most expensive part. Don't feel weird if it costs more than your CPU. Especially if you're using a SLI/Crossfire setup to use dual graphics cards. Here's a benchmark website also ran by PassMark https://www.videocardbenchmark.net/ SSD The next step for a gaming pc is in my opinion getting a solid state drive to use as your boot drive. You'll get near instantaneous boot ups as well do your best to eliminate the cinematic loading screens. (Skyrim so fast you won't be able to read the loading screen tips.) The main concern is to research the brand reliability and then get a good price on a reasonable size. Look for at least 80gb, Rockstar's GTA:5 for instance takes up 70gb. Shoot for a 120 or 240. Write/read speed is not so important nowadays as they're all so fast. A SSD also has no moving parts, less noise and heat. The only two downsides are price and potential for shorter lifespan, both of which are improving to make them almost negligible points. HDD No need for a guide. Buy as many as you need and what you need. What your computer can support is determined by your mobo slots and power supply connections. You could even get by without one. You may consider data redundancy through a Raid setup here, this would allow you to dig through the bargain bin of refurbished HDD's. Keep in mind more HDD's means more power, noise and heat. Power supply So about that. Don't get suckered into buying a 1,000w power supply unless you need one. Each part so far will list its power demand. Add them up and add 200w to be safe. Signs of insufficient power can be random shutdowns and screen flickering. A 500w is often adequate. Make sure to add power demands of two graphics cards if you're doing a SLI/Crossfire setup. The main choice is modularity. This means at the cheap end the power supply will include all the wires, but they can't be removed. Semi-modular means the primary cords cannot be removed but the ones for each additional HDD or whatever you're powering can be added or left off as needed. Fully modular, you get just a power box and all the cords you might need. Since they're all the same technically why spend an extra $100 or more to get modularity? Because when it comes to fitting your build into a case a tangled mess of cords can pose more than just an aesthetic issue. Go with the cheapest option for this build. Make it work! While building the case you might really regret saving money here, keep in mind. Case The case will list supported motherboard sizes. Chances are if you're building a gaming PC you want a mid size case at least and a mobo that isn't a Mini or Micro. The larger case the less dense the computer parts are and better cooling. Go for something simple yet elegant, this is a budget build remember! Cases usually include a fan or two. Spring for one if it's not included. Keep in mind your parts so far should already include a stock cooler fan for your CPU and your graphics card very likely has it's own fan(s). You can also construct builds where the case is the first concern like for compact rigs, or those very interesting custom cases like a guitar or a fish tank! RAM Don't get sold on RAM, 24gb of RAM won't make your computer run any faster. Based on game monitoring, 8gb of RAM is a good amount, consider more if you do large image file processing. Store brand 1600mhz is fine, try as I could I didn't find much reason to spring for 2133mhz RAM sticks with enormous cooling fins. (Those enormous cooling fins might not fit in your case.) It's also something you can go cheap on now and easily swap later. My accessories I included my accessories for no particular reason other than they've all worked for me great and were reasonably priced. A mechanical keyboard offers a snappier feeling of response but they can be thunderously loud, expect them to be heard over phone calls. The mouse cost me about $8. To each their own. The G230 Headphones are highly recommended. Seemed broken due to how quiet they are but after some adjusting with my mobo's audio software they ended up working fine. Not too heavy or tight on the ears. They are a closed headset. The mic is very high quality, but picks up some background noise even after adjustments. Well deserving of its reputation. \"Putting it together\" And there it was, your core shopping list to build your case. I hope what I've written is timeless enough to be relevant to you and you have the confidence to pick the parts to put together your own case. If your pc will be primarily used over wifi, grab a wifi adapter that is ac (the newer wifi technology that features a strong pinpointed signal over 5ghz bands). To get mine to install, first I followed directions on how to physically install in the case, then I downloaded the drivers from the company website and put on a USB to transfer to my new computer in order to get internet and finish setup. Or just plug in via LAN and go from there. Part by part installation I won't go into here because a video is the best way of learning this and I'm not doing videos at this time. I recommend Linus Tech Tips on youtube, he has some detailed videos that will get you going. Some things to keep in mind Use a anti-static wristband when handling loose electronics. Hooking the clip to your metal case can work. Handle everything as carefully as you can, you don't want to get into the installation procedures wondering if a software issue is actually a damaged motherboard. Computer sites will often charge you extra on super duper cooling paste for your cpu (an actual term is Enhanced Thermal Interface Material), but your cpu comes with some already applied. As Linus mentions, less can be more so be careful if applying your own. Note I didn't include a CD drive so none of the included discs (gpu, wlan card) were used, I downloaded the latest drivers from the hardware manufacturer websites. Format your boot partition as UEFI, this is the newest standard and offers quicker startup as well as will allow you to dual boot later on. I also recommend a GPT partition and not a MBR formatted partition. The first thing to do upon initial setup is to install updates, this can take quite a while. I'll also be writing a guide on OS installations, Windows or Linux, and for all of these I'll assume you have no physical media besides your own USB. Prioritizing your GPU and getting a SSD will give you a high tech experience. If that isn't enough then you might want to prepare for more advanced techniques like overclocking and liquid cooling and dual graphics cards. Generally the only need for this comes from playing games that haven't been finetuned or rendering multiple screens and using multiple intensive applications, gaming and live streaming for instance. Computer manufacturing has come a long way, you aren't expected to solder and most compatibilities are well managed for you. Hold onto your receipts, Microcenter (yes that wonderful store again) gives you the option of receiving your receipt by email as well as on paper. You'll need to keep track of your purchases when it comes to returns and keep hold of the boxes the parts came in when it comes to rebates etc. Have fun and stay in budget.","tags":"Hardware","url":"buying-guide-for-a-budget-gaming-pc.html"},{"title":"My first blog, I'll use Pelican!","text":"I realized learning things and making projects doesn't mean much if I can't talk about and display my work. So after some research I learned I don't need to specialize in front end development or website design or javascript to build a presentable website. This is part of the progress of software becoming more user friendly and accessible. Higher level languages and static page generators! There are some great articles about static pages vs. dynamic ones out there and I'm a total amateur so I'll just link one here . Why use a static blog? I'm interested in something that is quick, efficient, small footprint so whatever I ultimately produce can be viewed quickly and not waste time bogged down in a database heavy website. I don't have anything against a dynamic site, and my next project will be to make a django based one on AWS. For now though a simple blog meant to present data science projects can perfectly be done on a static site. As of now the blog is not finished, and I'm not an expert on everything I've done so far. This article is more of a journal on some of the things I've done so far. Some great resources here. Check out Nafiul Islam's pelican setup guide . Start-up Download Python which should include pip which is a package manager. Using pip, install Pelican. Use the quick set up, I only chose to say Yes to uploading it to a Github page one day. Once I had a test site I started looking for a theme. It's fun to learn by picking things apart, a lot better than digging around in docs and failing a million ways. Themes Pelican has a repository of default themes you can choose. They recommend downloading the whole thing which I did but now seems rather unnecessary. I started by using foundation-default-colours. I poked around in the theme but then found features in another blog I wanted to work with. So off to 'Plumage' I went. It's not a default theme but it was one I wanted to work with. Customization It took a few days to start getting something clean but customized. I changed the backgrounds to some Japanese inspired patterns that look subtle and organized to me. Changed the Pygment to the solarized light instead of the solarized dark, dark code background was too jarring. Then I found tango is a nice default Pygment so I compiled it to a css and inserted it. The theme was coded to option allow a Disqus comment section but after some research and just plain preference I decided to use a static comment system . This is a rather weird setup which won't scale well, but it's very nice for what I want now. No account needed to comment, full markdown support for comments, all personal information and data handling is performed by me not an independent service. The downside is comments aren't immediately posted and the stock handling is I have to review and add the comment manually. I may make this automatic but handling comments manually is appealing right now. Added a Show/Hide toggler courtesy of flavio . He combines it with the pelican plug-in pelican_gist which offers a simple Markdown tactic to insert blocks of text that can easily be hidden by the reader. Concise in the article write up, concise on the article page. Added pelican_gist which pulls up stored code from my gist. At this point the blog looks good enough to put up. I'm interested in continuing to push pelican/markdown some more so I'll keep poking around.","tags":"Projects","url":"my-first-blog-ill-use-pelican.html"},{"title":"How to install an openSUSE distro with dual boot","text":"I pulled out my old desktop and decided to put a linux distro on it so I could learn some popular database operating systems. Not sure I want to go into how to select one here, too many articles on it already. According to my research some good personal distros seem to be Ubuntu or Fedora, and some more commercial oriented ones (still free) are CentOS or openSUSE. A good paid one appears to be Redhat, CentOS is the freeware version (no support included). I decided to go with openSUSE, but I will likely be working with CentOS as well. Dual boot openSUSE This desktop was one I built and had installed Win7 with an eventual Win10 upgrade. The goal therefore was to install openSUSE alongside my existing Windows 10. To make a long story short, this turned out to be impossible as my main drive was MBR formatted. What I needed to dual boot easily was a GPT formatted drive. Here is an article discussing the differences. Now there are some unofficial utilities that claim to migrate files in a reformat safely. But the confidence wasn't there so I decided to do a rescue operation of the C drive, take out everything I wanted and just reformat entirely to GPT. I have a laptop that I used to write a openSUSE distro in USB boot drive. I chose Leap as it is the stable released version they offer. They also offer Tumbleweed which is the more experimental rolling version featuring the latest, also claimed to be stable. Installation I wrote to the usb using Rufus which specializes in creating bootable USB drives. Need to find your Windows key? Check out this VB script courtesy of thewindowsclub. Here is the code itself. Open a text file and paste in this code and save as a .vbs file. Then just click the file to generate it. I should note here are some simpler methods. Show/hide Windows key finder vbs script Set WshShell = CreateObject(\"WScript.Shell\") MsgBox ConvertToKey(WshShell.RegRead(\"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\DigitalProductId\")) Function ConvertToKey(Key) Const KeyOffset = 52 i = 28 Chars = \"BCDFGHJKMPQRTVWXY2346789\" Do Cur = 0 x = 14 Do Cur = Cur * 256 Cur = Key(x + KeyOffset) + Cur Key(x + KeyOffset) = (Cur \\ 24) And 255 Cur = Cur Mod 24 x = x -1 Loop While x >= 0 i = i -1 KeyOutput = Mid(Chars, Cur + 1, 1) & KeyOutput If (((29 - i) Mod 6) = 0) And (i <> -1) Then i = i -1 KeyOutput = \"-\" & KeyOutput End If Loop While i >= 0 ConvertToKey = KeyOutput End Function Once you have a copy of Windows that corresponds to what you are licensed to use, you might want to go ahead and find your windows key for after the installation when you need to reactivate. If you're simply wiping your drive and reinstalling then Windows will very likely automatically activate your copy. They monitor your hardware signature and will approve it if nothing has changed drastically. You may expect some hassle if some parts change and have to speak to customer service if a lot has changed. I don't know the entire policy since thankfully I didn't need to use my product key as it was automatically activated. But you should follow earlier directions and store your Windows key just in case. But wait, having trouble getting a Windows copy? Windows really doesn't want to serve their OS for people, I imagine the server costs would be unfathomable. So almost no matter how botched your copy is or what weird situation you're in, Windows doesn't want to give you a copy, they just keep improving their reinstall software. There is a solution, check out Windows ISO Downloader from heidoc It worked great for me. Armed with a windows key and a copy of Windows, I pulled the trigger and reformatted my C drive completely. (I am working on a SSD here for anyone curious.) Ok now that you have your two USB boot drives, one with Windows, one with your linux distro. (You could also just use the same one, but two is simpler). Install Windows first, openSUSE does a decent job of detecting an existing OS and Windows is territorial. Alright you've installed Windows. That was the easy part. Now put in your linux USB and boot from it. Since you went through the trouble of ensuring a GPT format, this should be rather easy. Choosing the recommended Btrfs file format will create an almost countless assortment of partitions. Sticking with ntfs cuts it down to 3 partitions. The option you chose when creating your USB with Rufus should've specified UEFI for the boot drive, it's the default. Once you finish the installation prompts for both operating systems you will face an option upon booting up to choose which installed boot drive to access. My openSUSE is the default which starts after a countdown. Well that was it, wasn't too bad. I use KDE which is a nice gui or \"desktop environment\" and the recommended one for openSUSE, haven't found an error yet.","tags":"Hardware","url":"how-to-install-an-opensuse-distro-with-dual-boot.html"}]}
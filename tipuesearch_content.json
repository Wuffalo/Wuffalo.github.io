{"pages":[{"title":"About Me","text":"Contact me: contact.wolf.site@gmail.com Hi and welcome to my blog! My name is Wolfgang, and I'm a tech worker in Texas. I have a B.S. with specializations in Production Management and German studies. I am interested in data science and all the tools that help me analyze the world. I have a working proficiency in Python and a decent knack for figuring out anything I need. Besides some data science projects I am improving my database and server skills. About Blog This blog was generated using Pelican, a python language static site generator, and the theme is based off of Kevin Deldycke's 'Plumage' theme. If at any time your experience on this blog is slow or anything unsecure appears, let me know on my blog's Github or by email. There you can check out my source html as well as the javascripts I use. I have added a static comment section, feel free to post a comment on any article using Markdown syntax if you would like. This requires your browser to have a Mailto setup as the comment is in fact emailed to me for approval. I like this system because it allows you to remain anonymous, use advanced syntax and the interaction is entirely between us with no 3rd party software. How flexible is Markdown? I use Markdown to format my posts. Topics I'll try and stick to tips and tricks I come across. In my Hardware section you will find computer hardware projects. In my Projects section you will find software and data projects. I've added a Software section for pure software articles. My Personal might be interesting to you.","tags":"pages","url":"about-me/"},{"title":"Reduce API calls and process time- or How I learned to pickle my quandl","text":"I've written about Quandl before and the wealth of information they provide in an easy to process way. However their generosity does know limits. While it's difficult to brush up against their daily call limits, I might as well do what I can to be respectful of their service and my efficiency. Introducing another Python package to discuss- Pickle . In short pickle allows python objects to be converted to streams and stored, then later accessed. The concept behind this is serialization. This also would allow objects to be transmitted to sockets and reproduced accurately, although pickle versioning is important. Pickle does not provide any form of security, all pickles imported are assumed to be in working order and not tampered with. Using pickle allows me to store my python dataframes and open them from their binary rather than make additional calls to quandl. The most immediate benefit is speed. This program from a previous article is now run entirely offline. Full Program here Show/hide Code #!/usr/bin/env python3 # -*- coding: utf-8 -*- #!/usr/bin/env python3 # -*- coding: utf-8 -*- \"\"\" Effective Federal Funds Rate over time with economic recessions @author: wolf \"\"\" import pickle import quandl import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) quandl . ApiConfig . api_key = 'your-api-key-here' #-----#Uncomment following block to pull and create updated dataframes #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" , start_date = \"1955-01-01\" ) output = open ( \"EffFundsRate.pickle\" , 'wb' ) pickle . dump ( df2 , output , pickle . HIGHEST_PROTOCOL ) output . close () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) output = open ( \"FREDrecession.pickle\" , 'wb' ) pickle . dump ( df4 , output , pickle . HIGHEST_PROTOCOL ) output . close () #------#Uncomment following block to load stored dataframes pickle_in = open ( 'EffFundsRate.pickle' , 'rb' ) df2 = pickle . load ( pickle_in ) pickle_in . close () pickle_in = open ( 'FREDrecession.pickle' , 'rb' ) df4 = pickle . load ( pickle_in ) pickle_in . close () #------# df2 [ 'Value' ] . plot () maxY = df2 [ 'Value' ] . max () y2 = df4 [ 'Value' ] plt . fill_between ( df4 . index , 0 , maxY , where = y2 == 1 , facecolor = 'blue' , alpha = 0.2 ) plt . xlabel ( 'Year' ) plt . title ( 'Federal Funds effective rate with recession times shaded' ) plt . show () Note: I swapped my unique API key with a placeholder, you can run this code by deleting that line to access Quandl as an unregistered user. As an unregistered user you have lower call limits. Key points For now the source is a manual choice of commenting one block or the other. Both blocks run uncommented works but this is a complete reset, as it pulls fresh data and then overwrites the old pickle files. To run a fresh pull from quandl and write new pickle files the first block is run: #-----#Uncomment following block to pull and create updated dataframes #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" , start_date = \"1955-01-01\" ) output = open ( \"EffFundsRate.pickle\" , 'wb' ) pickle . dump ( df2 , output , pickle . HIGHEST_PROTOCOL ) output . close () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) output = open ( \"FREDrecession.pickle\" , 'wb' ) pickle . dump ( df4 , output , pickle . HIGHEST_PROTOCOL ) output . close () Once this has been performed it doesn't need to be rerun unless the source data has updated. (In this case I am pulling weekly data updated every Wednesday and collapsed by month.) After commenting out the previous block I can now uncomment the following: #------#Uncomment following block to load stored dataframes pickle_in = open ( 'EffFundsRate.pickle' , 'rb' ) df2 = pickle . load ( pickle_in ) pickle_in . close () pickle_in = open ( 'FREDrecession.pickle' , 'rb' ) df4 = pickle . load ( pickle_in ) pickle_in . close () #------# This block is now all I need to run to import my stored pickle data as a python object, as originally stored. Now I can enjoy fast 'free' data that is stored offline. Improvements: The use cases I've seen all used this commenting/uncommenting strategy, but this is a choice that would have to be made by a programmer. To make this more user friendly I could have a user prompt at the beginning when run on a command line or window asking, \"Would you like to refresh to latest source data?\" Otherwise you could be at risk of using outdated data when quandl was chosen for its up to the minute accuracy.","tags":"Software","url":"reduce-api-calls-and-process-time-or-how-i-learned-to-pickle-my-quandl.html"},{"title":"Shading a federal funds rate graph by variable","text":"There's some talk in the news about economic policy at the federal reserve taking a turn to start increasing the federal funds rate. Fed president Janet Yellen has indicated on what conditions she will raise it and it appears increasingly likely. Raising rates is meant to tamp down inflation, but shouldn't it also quiet down investment and economic growth? That's an open question and to help answer that I wanted to graph both federal funds rate and years of recession. To do this I'm using Quandl's data sets, specifically their Federal Funds Effective Rate and their Recession Indicators . Here is the code I'm using: Show/hide Code import quandl import matplotlib.pyplot as plt from matplotlib import style style . use ( 'ggplot' ) quandl . ApiConfig . api_key = 'my_api_key_here' #Effective Federal Funds Rate df2 = quandl . get ( \"FED/RIFSPFF_N_WW\" , collapse = \"monthly\" ) df2 [ 'Value' ] . plot () #Recession, 1=recession df4 = quandl . get ( \"FRED/USRECP\" , start_date = \"1955-01-01\" ) maxY = df2 [ 'Value' ] . max () y2 = df4 [ 'Value' ] plt . fill_between ( df4 . index , 0 , maxY , where = y2 == 1 , facecolor = 'blue' , alpha = 0.2 ) plt . xlabel ( 'Year' ) plt . title ( 'Federal Funds effective rate with recession times shaded' ) plt . show () Note: I swapped my unique API key with a placeholder, you can run this code by deleting that line to access Quandl as an unregistered user. As an unregistered user you have low call limits. So what does my code produce? Exactly what I wanted of course, as you can see here: Full confession, this isn't the most unique connection to make, and it's one that's largely uncorrelated. Most of the fun here is just figuring out how to shade under and over a line to replicate an effect often used by Financial Times and others. The key part is making it variable based on the data itself. That'll be useful for later. Now for a real world example, here is one from the St. Louis Federal Reserve .","tags":"Software","url":"shading-a-federal-funds-rate-graph-by-variable.html"},{"title":"Weekend reading: Quandl tables, Django with Heroku","text":"I've been following some machine learning tutorials for Python by Sentdex on YouTube. He's notable for his prolific output of Python tutorials on various topics and for making the topics reachable to all audiences. He has tutorial series in financial analysis, python skills and every sort of interesting thing Python can do. Here's his machine learning series that I'm working through: Quandl Sentdex pulled sample data from Quandl using their Py module . Their website includes an API section to get you started pulling the data using whatever language or software you prefer. What is Quandl? Simply put, it's one of the coolest data providers I've come across! They mainly deal in financial data, including End Of Day stock results and fundamental indicators of company and economic health. That's just the beginning, they made a name for themselves by attracting alternative data as different as satellite measurements of global oil tank storage levels to national UFO sightings (yes the last one is real, but the jury is out on whether it's real .) The data I was most interested in was housing data from Zillow, Federal Reserve data on the economy, European Central Bank records and even China Macroeconomic and Industrial Data. It's invigorating to see 'live' data that I can easily pull in and play with and not just read summary articles by others. Will I gain insights? At this point I highly doubt it, however it's a great playground for machine learning and maybe one day I'll contribute some ideas to how things work. Heroku What is Heroku ? It's technically referred to as a PaaS (Platform as a Service), meaning they provide not just a server but a managed software environment to host your apps. They take care of scaling (based on your specifications), database management and all the background infrastructure problems which could take you down. All you do is bring your app and manage it. I'm looking at some new web service projects and Heroku stood out for its popular usage. If languages can be lower or higher level based on their complexity then PaaS could be called higher level for doing more to manage infrastructure than IaaS providers do. Infrastructure as a Service (IaaS) provides the server backbone to host your own software. PaaS providers handle software licenses and other sysadmin duties for you. Well maybe you already know this. I followed Heroku's free tutorial which featured downloading their CLI and uploading a premade Django web app. It honestly wasn't too complicated in concept and pretty familiar to my blog development process. Developing an app, applying git version control to track progress and push updates. The interesting parts were Heroku's management software, hosting your app locally, spinning up your one Free dyno to serve it from Heroku. All in all they seem to do a good job of separating the infrastructure from app development, and I wouldn't mind to work with them. That was the weekend!","tags":"Personal","url":"weekend-reading-quandl-tables-django-with-heroku.html"},{"title":"BRS Part 2","text":"This is part two of my Blended Retirement System study. For more information please see my original article, BRS Part 1 . Now that I've made a sample program which can take a user input and provide the correct pay rate, I'm going to do two things: convert my code to Python 3 (3.6) and begin creating a simulation program. The original program was written in Python 2.7 which is a legacy standard still widely used. However since I have the luxury of forward planning I will go ahead and switch to Python 3 before it becomes burdensome. I did experiment with some code conversion methods that promise to mostly automate the process, but it quickly became apparent I was just bloating my simple program. Therefore I just made the necessary changes, input and print updates. CalcPay in Python 3 Show/hide Code #!/usr/bin/env python3 \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import sys import math import pandas as pd def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print ( \"Service years was entered incorrectly. Must be >0 and <40.\" ) return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" separator = ' ' print ( \"Please enter rank followed by years in service.\" ) print ( \"Format is 'E-1' space #, ex: 'O-5 20' or 'E-3 6'\" ) user_input = input ( \": \" ) . split ( separator ) rank = user_input [ 0 ] service = user_input [ 1 ] active_input = input ( \"Active duty, Y? Reserve is N.: \" ) . lower () if active_input in ( \"yes\" , \"y\" ): active = 1 act_out = 'n active duty ' elif active_input in ( \"no\" , \"n\" ): active = 0 act_out = ' reserve ' else : sys . exit ( \"Please try again with Y or N for active duty answer.\" ) enlist = str ( user_input [ 0 ]) if ord ( enlist [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 rank = str ( rank ) service = int ( service ) years = determine_years ( service ) pay = find_pay ( rank , years , active , enlisted ) if math . isnan ( pay ): print ( \"There doesn't exist a base pay for that rank with those years of service\" ) else : print ( \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay )) if __name__ == \"__main__\" : calc_pay () Simulation Now that my basic program is converted to Python 3 I can start playing with it for the purpose of this project. As a reminder the ultimate goal is to analyze cost savings of the US military's implementation of the new Blended Retirement System. I decided to create a simulator and it is here I am running into disappointment. The goal is to create a simulator that very closely mirrors reality with accurate distributions in costs as calculated by numbers in each rank and pay rate. Here is my simulator code so far: Show/hide Code #!/usr/bin/env python3 \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import math import random import pandas as pd import numpy as np def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print ( \"Service years was entered incorrectly. Must be >0 and <40.\" ) return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" active = random . randrange ( 2 ) enlisted = random . randrange ( 2 ) rank1 = pd . read_csv ( 'usarmy2017.csv' ) rank_df = pd . DataFrame ( rank1 ) rank_list = rank_df [ 'rank' ] percent_list = rank_df [ 'percent' ] rank = np . random . choice ( rank_list , p = percent_list ) min_year = ( rank_df [ rank_list == rank ]) . iloc [ 0 , 3 ] service = random . randrange ( min_year , 41 ) years = determine_years ( service ) if ord ( rank [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 pay = find_pay ( rank , years , active , enlisted ) if active == 1 : act_out = 'n active ' else : act_out = ' reserve ' if math . isnan ( pay ): print ( \"There doesn't exist a base pay for a \" + rank + \" with \" + str ( service ) + \" years of service.\" ) else : print ( \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay )) if __name__ == \"__main__\" : calc_pay () You'll see I have eliminated user inputs, now the inputs are randomized with appropriate possible inputs. This program is still producing just one data point so far. To base my simulator in reality I am weighing the randomness by real world amounts by accessing a csv file with numpy.random.choice() . This selects a random rank based on its weighted distribution provided in the chart. You can view the chart here. Please note I have decided to narrow my development to only look at US army ranks for now. This simulator should eventually be able to work for all services and eventually return any year analysis. That's going to be a lot of csv tables! Or just redesign to operate off a database, right now I am working with PostGreSQL in other projects. Notice the first weight is provided in exponent notation which is well interpreted by NumPy. However you likely noticed the last column minyears . This column provides the commonly understood minimum number of years of service to achieve the corresponding rank. It's not a perfect science since there are performance factors involved, and it also breaks down in the upper officer ranks where they aren't clear rules. What I have created however is a lower bounds for my service years. Where now? At this point my data is starting to become imperfect and I need to pause and analyze how far I can take this simulator. I have a perfect distribution for rank. I can also create perfect distributions for questions of enlisted/officer or active/reserve. That would create a perfect simulation, but unfortunately I need to have the crucial input of service years. The only way to fix this I see is to find data of experience by rank. I've been digging through actuary.defense.gov to see their yearly reports, but haven't found something this exact yet. So I am naturally going to take this two ways at once, keep digging for real numbers, but also begin constructing a solve-for that could populate this for me using the total costs against all the known inputs I have so far. The only variable missing is a statistical distribution of service by rank. If I create this I can then achieve a most perfect simulator. However this also makes me want to work on other projects.","tags":"Projects","url":"brs-part-2.html"},{"title":"BRS Part 1","text":"Blended Retirement System costs, financial analysis of the US Military's new retirement system effective 2018 This is a multi-stage analysis of the new retirement system for all military personel going into effect January 1, 2018. Part one is a programming project in Python adding a calculator to the Department of Defense's website at militarypay.defense.gov Under the previous system retirement was only awarded after 20 years of service, either active duty years or a combination including reserve 'good years'. A good year defined as meeting sufficient reserve service requirements. The new system will be a combination of pension, bonus and investment fund matching. Since only 17% of service members stay to retire after 20 years, this new system will affect and potentially benefit the majority of service members. Calculator The calculator takes in multiple pay tables saved as csv files, one of the simplest data storage formats. These tables therefore take up minimal space and can be read and wrote to by any system. The advantage of this calculator is it allows the user to retrieve reserve or active duty pay rates. User input determines which pay table is relevant and which particular pay on the chart to return. Code The program to do this is written in Python and the code is as follows with corresponding commenting. Show/hide \"\"\"Calculate 2017 US military service base pay. By Wolf\"\"\" import sys import math import pandas as pd def find_pay ( rank , years , active , enlisted ): \"\"\"Returns cell value given rank, years, duty status, enlisted status\"\"\" if active == 1 : if enlisted == 1 : ae_read = pd . read_csv ( 'E_2017pay.csv' ) ae1 = pd . DataFrame ( ae_read ) a_e = ae1 . set_index ( 'Years' ) pay = a_e . loc [ years , rank ] return pay elif enlisted == 0 : ao_read = pd . read_csv ( 'O_2017pay.csv' ) ao1 = pd . DataFrame ( ao_read ) a_o = ao1 . set_index ( 'Years' ) pay = a_o . loc [ years , rank ] return pay elif active == 0 : if enlisted == 1 : re_read = pd . read_csv ( 'RE_2017pay.csv' ) re1 = pd . DataFrame ( re_read ) r_e = re1 . set_index ( 'Years' ) pay = r_e . loc [ years , rank ] return pay elif enlisted == 0 : ro_read = pd . read_csv ( 'RO_2017pay.csv' ) ro1 = pd . DataFrame ( ro_read ) r_o = ro1 . set_index ( 'Years' ) pay = r_o . loc [ years , rank ] return pay def determine_years ( service ): \"\"\"Takes years of service, choose experience range assigns to years\"\"\" if 0 <= service < 2 : years = 'Under 2' elif 2 <= service < 3 : years = 'Over 2' elif 3 <= service < 4 : years = 'Over 3' elif 4 <= service < 6 : years = 'Over 4' elif 6 <= service < 8 : years = 'Over 6' elif 8 <= service < 10 : years = 'Over 8' elif 10 <= service < 12 : years = 'Over 10' elif 12 <= service < 14 : years = 'Over 12' elif 14 <= service < 16 : years = 'Over 14' elif 16 <= service < 18 : years = 'Over 16' elif 18 <= service < 20 : years = 'Over 18' elif 20 <= service < 22 : years = 'Over 20' elif 22 <= service < 24 : years = 'Over 22' elif 24 <= service < 26 : years = 'Over 24' elif 26 <= service < 30 : years = 'Over 26' elif 30 <= service < 34 : years = 'Over 30' elif 34 <= service < 38 : years = 'Over 34' elif 38 <= service <= 40 : years = 'Over 38' elif service < 0 and service > 40 : print \"Service years was entered incorrectly. Must be >0 and <40.\" return years def calc_pay (): \"\"\"Opens Calculator prompt\"\"\" separator = ' ' #separator of input values is space print \"Please enter rank followed by years in service.\" print \"Format is 'E-1' space #, ex: O-5 20 or 'E-3 6'\" user_input = raw_input ( \": \" ) . split ( separator ) #reads in rank and service years rank = user_input [ 0 ] service = user_input [ 1 ] active_input = raw_input ( \"Active duty, Y? Reserve is N.: \" ) . lower () #reads active duty or not if active_input in ( \"yes\" , \"y\" ): #Assigns rest of program's active/reserve references active = 1 act_out = 'n active duty ' elif active_input in ( \"no\" , \"n\" ): active = 0 act_out = ' reserve ' else : sys . exit ( \"Please try again with Y or N for active duty answer.\" ) enlist = str ( user_input [ 0 ]) #This body determines whether user's rank is enlisted or officer if ord ( enlist [ 0 ]) == ord ( 'E' ): enlisted = 1 else : enlisted = 0 rank = str ( rank ) #Prepares user inputs for function calls with correct data types service = int ( service ) years = determine_years ( service ) #Takes user's service years and assigns experience pay range pay = find_pay ( rank , years , active , enlisted ) #Looks up correct chart and cell value for rank, years in, active or reserve, and enlisted or officer if math . isnan ( pay ): #Prints a summary statement if there exists a base pay for user input, or an error message print \"There doesn't exist a base pay for that rank with those years of service\" else : print \"The 2017 base pay for a\" + act_out + rank + \" with \" + str ( service ) + \" years of service is: \" , '${:,.2f}' . format ( pay ) if __name__ == \"__main__\" : calc_pay () Screenshots Here is a snapshot of the pay table for active duty enlisted soldiers represented in raw CSV Here is a snapshot of the pay table for reserve officers automatically formatted in an excel application Here is the input prompt when the program is ran Following the first input there is a secondary input, after both are entered the ouput lists the pay rate for the selected rank, years of experience and active or reserve duty selection If an input follows appropriate formatting but doesn't choose a valid pay rate, an error note is returned Conclusion The next project in this series will be data analysis to examine system wide costs and savings.","tags":"Projects","url":"brs-part-1.html"},{"title":"Measuring program time (timeit module)","text":"I've been using python's timeit module for some testing, wanted to keep track here of timeit and any other methods I come across. Here's an example usage. import timeit start = timeit . default_timer () # stop = timeit . default_timer () print ( stop - start ) Note: default_timer uses time.perf_counter() as defined here Return the value (in fractional seconds) of a performance counter, i.e. a clock with the highest available resolution to measure a short duration. It does include time elapsed during sleep and is system-wide. The reference point of the returned value is undefined, so that only the difference between the results of consecutive calls is valid. Linux Since I am using linux I can also use a built in bash command of 'time' and run the python program from the command line. $ time python program.py This will return output similar to the following. real 0m4.139s user 0m0.301s sys 0m0.053s A good explanation is found here on stackoverflow . Real is wall clock time - time from start to finish of the call. This is all elapsed time including time slices used by other processes and time the process spends blocked (for example if it is waiting for I/O to complete). User is the amount of CPU time spent in user-mode code (outside the kernel) within the process. This is only actual CPU time used in executing the process. Other processes and time the process spends blocked do not count towards this figure. Sys is the amount of CPU time spent in the kernel within the process. This means executing CPU time spent in system calls within the kernel, as opposed to library code, which is still running in user-space. Like 'user', this is only CPU time used by the process. See below for a brief description of kernel mode (also known as 'supervisor' mode) and the system call mechanism. Here is a good article about formatting options . Summary, time is a good benchmarking tool I will be using from the command line, and timeit is one I can insert into my programs and measure any segment I need. timeit Documentation here . Addendum; Here's an example usage from my program I wrote about in this blog post . I used a timeit start at the beginning of the program in the declarations, then I stopped it at the end of the main body after the final print statements. It gives me the first time value of 3.43 seconds, showing that real time measured of the main body of program was 3.43 seconds. Real time includes user prompt and my typing time! The next time values come from bash's time command, so its real time returned 3.75 seconds. Now this should be comparable to the timeit I ran from within the program, however the bash real will always be greater as it is run outside the program, so it also counts time to open and close the program as well as other boring system things like import statements. So far I haven't seen a time measure that doesn't include time I spent typing, and that's where the next two values are very useful. They come from bash's time, and they measure process time itself, see details above. So my user mode used my processor for 0.3 seconds (program management) and kernel calculations took 0.026 seconds. The kernel time is the best for comparing actual work performed. I can use this to compare to other programs.","tags":"Software","url":"measuring-program-time-timeit-module.html"},{"title":"Why overspend for smoothies?","text":"Smoothies are liquid snacks that generally taste great, and an easy way to pack in what you should be having. Now if you search Costco for smoothie mix you'll find a bag of https://www.raderfarms.com/ fresh start smoothie mix in a 3 lbs (48 oz) bag sold for around $12. Inside are 6 pouches of 8oz each. Fresh Start smoothie blend While it's a very nice mix, I've switched over to just making my own. Still shopping at Costco but now I am combining two other products sold to make a similar mix. The new mixes Earthbound Farm Power Green That's 1.5 lbs (24 oz) of Spinach, Kale and chard and Kirkland Three Berry Blend Here we've got 4 lbs (64 oz) of Blackberries, Blueberries and Raspberries Product lb oz price $/oz Rader Farm smoothie mix 3 48 12 .25 Earthbound Farm Power Greens 1.5 24 4 .17 Kirkland Three Berry Blend 4 64 7 .11 Ok just to play some more with the numbers assuming the Rader Farm smoothie mix is 50/50 greens and berries by weight, then the simple combined $/oz price for my substitute mix is .14 which is compared to the .25 of the smoothie mix. So you're paying 78% more for the prepackaged baggies of smoothie mix. Notes: You lose out on strawberries but gain blackberries. Also the smoothie blend kale is mature and the Earthbound is baby kale. Go forth and make your own smoothie mix!","tags":"Personal","url":"why-overspend-for-smoothies.html"},{"title":"Buying Guide for a budget gaming PC","text":"Goals: This is a gaming PC that can play most modern games on the maximum settings. This is also a self-build and every cost saving is used. This guide is a bit outdated since I built this a few years ago, the focus is to show how to price check and some buying recommendations as well as some installation advice. Also note this is written for a beginner level. Specifications: Part Name known as Brand Model Central Processing Unit CPU Intel i5-4690k Motherboard MOBO MSI Kraitos Graphics Processing Unit GPU AMD R9 290 4GB DCUII Solid State Drive SSD Crucial 240GB mx300 Harddrive HDD Western Digital 1TB Green internal Random Access Memory RAM 8gb 2 sticks of 4GB 1600mhz Case box NZXT S340 Accessory Brand/Model Monitor Asus VN 248 Keyboard TTesports Knucker Mouse Cobra E-3lue Headphones Logitech G230 Budget The first thing to check is your budget, a good way of getting an idea is look at some other builds and start getting an idea of the hardware you can afford in your budget. Some good sites to check are: Reddit's /r/buildapc 'Paul's Hardware' on YouTube and of course some prebuilt PC sites like Main Gear Don't be intimidated by some of the costs of $2,000+ PC builds, those tend to include dual graphics cards, custom cooling systems, the absolute best CPU and many bizarre addons like engravings, cutouts and LED lightups. No matter how aggressively styled your computer case is it won't make you click your mouse any faster. This guide assumes you're going for more of a barebones approach to a gaming PC and your home is room temperature. I also hope that the advice here is relevant to people building low end computers or one of those really high end monsters. Ok so now you have a better idea of your budget, I'm thinking under $1,000 is a good goal. As time goes on this should be more achievable. Hopefully. CPU The first thing to look at is your CPU, this determines your motherboard (MOBO). i5 or i7 processers use different socket architecture for instance, when buying a CPU look at socket (e.g. LGA1150 or LGA1151) to determine which mobo you can choose. I recommend Intel for power efficiency but not necessarily for cost and power. The trick here is to find a good deal. Keep in mind prebuilt computers won't include as many hardware deals that you can get shopping individually. This is where I start selling you on - Microcenter. Now I know locations vary and you might not even have one in your country. If Microcenter is an option be sure to include them as a key price checker. A good way to start that is stop by a store and pick up their product newspaper. There they'll list all the schemes of mixing and matching hardware to get instant discounts. Be careful when sniping off one piece at a time from Newegg or eBay as you'll miss out on deals like \"$100 off for buying a CPU and GPU together\". The meat of the matter is benchmarks. Intel and AMD create different CPU's so to learn the differences you'll have to check out benchmarks. This will be your own journey. An excellent website is cpubenchmark.net which allows you to compare CPUs as shown here. The CPU mark is a method of comparison. Keep in mind however you don't need the latest top of the line CPU, Moore's Law dictates you will increasingly be ripped off when it comes to building gaming PC's to run Hotline Miami. My i5-4690k manages to run games on max settings without having to full load and creating a necessity for cooling solutions. The stock cooler therefore works fine and that's a saving. Why doesn't my CPU get taxed by individual games? The GPU is the powerhouse that runs the games, which are chiefly graphically taxing. Don't expect too much from a lower end processer, but also don't expect your i7-7700 to need a $300 liquid cooling pump (with pulsating LEDs). Was it improper to discuss the GPU in the CPU section? Not at all. The GPU is the fundamental hardware for a gaming PC (budgetary concern), but the CPU is the fundamental hardware for a PC (technical concern). In other words your CPU determines other parts, but your GPU is independent and is also the most important part of your gaming rig. MOBO Now that you've picked your CPU you can look at corresponding motherboards. A CPU will list the socket it requires. The motherboard is the backbone for all the hardware you buy. I chose a MSI Kraitos due to the package deal offered with the CPU at Microcenter. The highlights are GPU This is the key decision point for your build, you can select from a wide range of cards and it'll be hard to tell what's right for you. The best way is to look at your budget and understand the GPU is really what pushes performance. In other words this will be your most expensive part. Don't feel weird if it costs more than your CPU. Especially if you're using a SLI/Crossfire setup to use dual graphics cards. Here's a benchmark website also ran by PassMark https://www.videocardbenchmark.net/ SSD The next step for a gaming pc is in my opinion getting a solid state drive to use as your boot drive. You'll get near instantaneous boot ups as well do your best to eliminate the cinematic loading screens. (Skyrim so fast you won't be able to read the loading screen tips.) The main concern is to research the brand reliability and then get a good price on a reasonable size. Look for at least 80gb, Rockstar's GTA:5 for instance takes up 70gb. Shoot for a 120 or 240. Write/read speed is not so important nowadays as they're all so fast. A SSD also has no moving parts, less noise and heat. The only two downsides are price and potential for shorter lifespan, both of which are improving to make them almost negligible points. HDD No need for a guide. Buy as many as you need and what you need. What your computer can support is determined by your mobo slots and power supply connections. You could even get by without one. You may consider data redundancy through a Raid setup here, this would allow you to dig through the bargain bin of refurbished HDD's. Keep in mind more HDD's means more power, noise and heat. Power supply So about that. Don't get suckered into buying a 1,000w power supply unless you need one. Each part so far will list its power demand. Add them up and add 200w to be safe. Signs of insufficient power can be random shutdowns and screen flickering. A 500w is often adequate. Make sure to add power demands of two graphics cards if you're doing a SLI/Crossfire setup. The main choice is modularity. This means at the cheap end the power supply will include all the wires, but they can't be removed. Semi-modular means the primary cords cannot be removed but the ones for each additional HDD or whatever you're powering can be added or left off as needed. Fully modular, you get just a power box and all the cords you might need. Since they're all the same technically why spend an extra $100 or more to get modularity? Because when it comes to fitting your build into a case a tangled mess of cords can pose more than just an aesthetic issue. Go with the cheapest option for this build. Make it work! While building the case you might really regret saving money here, keep in mind. Case The case will list supported motherboard sizes. Chances are if you're building a gaming PC you want a mid size case at least and a mobo that isn't a Mini or Micro. The larger case the less dense the computer parts are and better cooling. Go for something simple yet elegant, this is a budget build remember! Cases usually include a fan or two. Spring for one if it's not included. Keep in mind your parts so far should already include a stock cooler fan for your CPU and your graphics card very likely has it's own fan(s). You can also construct builds where the case is the first concern like for compact rigs, or those very interesting custom cases like a guitar or a fish tank! RAM Don't get sold on RAM, 24gb of RAM won't make your computer run any faster. Based on game monitoring, 8gb of RAM is a good amount, consider more if you do large image file processing. Store brand 1600mhz is fine, try as I could I didn't find much reason to spring for 2133mhz RAM sticks with enormous cooling fins. (Those enormous cooling fins might not fit in your case.) It's also something you can go cheap on now and easily swap later. My accessories I included my accessories for no particular reason other than they've all worked for me great and were reasonably priced. A mechanical keyboard offers a snappier feeling of response but they can be thunderously loud, expect them to be heard over phone calls. The mouse cost me about $8. To each their own. The G230 Headphones are highly recommended. Seemed broken due to how quiet they are but after some adjusting with my mobo's audio software they ended up working fine. Not too heavy or tight on the ears. They are a closed headset. The mic is very high quality, but picks up some background noise even after adjustments. Well deserving of its reputation. \"Putting it together\" And there it was, your core shopping list to build your case. I hope what I've written is timeless enough to be relevant to you and you have the confidence to pick the parts to put together your own case. If your pc will be primarily used over wifi, grab a wifi adapter that is ac (the newer wifi technology that features a strong pinpointed signal over 5ghz bands). To get mine to install, first I followed directions on how to physically install in the case, then I downloaded the drivers from the company website and put on a USB to transfer to my new computer in order to get internet and finish setup. Or just plug in via LAN and go from there. Part by part installation I won't go into here because a video is the best way of learning this and I'm not doing videos at this time. I recommend Linus Tech Tips on youtube, he has some detailed videos that will get you going. Some things to keep in mind Use a anti-static wristband when handling loose electronics. Hooking the clip to your metal case can work. Handle everything as carefully as you can, you don't want to get into the installation procedures wondering if a software issue is actually a damaged motherboard. Computer sites will often charge you extra on super duper cooling paste for your cpu (an actual term is Enhanced Thermal Interface Material), but your cpu comes with some already applied. As Linus mentions, less can be more so be careful if applying your own. Note I didn't include a CD drive so none of the included discs (gpu, wlan card) were used, I downloaded the latest drivers from the hardware manufacturer websites. Format your boot partition as UEFI, this is the newest standard and offers quicker startup as well as will allow you to dual boot later on. I also recommend a GPT partition and not a MBR formatted partition. The first thing to do upon initial setup is to install updates, this can take quite a while. I'll also be writing a guide on OS installations, Windows or Linux, and for all of these I'll assume you have no physical media besides your own USB. Prioritizing your GPU and getting a SSD will give you a high tech experience. If that isn't enough then you might want to prepare for more advanced techniques like overclocking and liquid cooling and dual graphics cards. Generally the only need for this comes from playing games that haven't been finetuned or rendering multiple screens and using multiple intensive applications, gaming and live streaming for instance. Computer manufacturing has come a long way, you aren't expected to solder and most compatibilities are well managed for you. Hold onto your receipts, Microcenter (yes that wonderful store again) gives you the option of receiving your receipt by email as well as on paper. You'll need to keep track of your purchases when it comes to returns and keep hold of the boxes the parts came in when it comes to rebates etc. Have fun and stay in budget.","tags":"Hardware","url":"buying-guide-for-a-budget-gaming-pc.html"},{"title":"My first blog, I'll use Pelican!","text":"I realized learning things and making projects doesn't mean much if I can't talk about and display my work. So after some research I learned I don't need to specialize in front end development or website design or javascript to build a presentable website. This is part of the progress of software becoming more user friendly and accessible. Higher level languages and static page generators! There are some great articles about static pages vs. dynamic ones out there and I'm a total amateur so I'll just link one here . Why use a static blog? I'm interested in something that is quick, efficient, small footprint so whatever I ultimately produce can be viewed quickly and not waste time bogged down in a database heavy website. I don't have anything against a dynamic site, and my next project will be to make a django based one on AWS. For now though a simple blog meant to present data science projects can perfectly be done on a static site. As of now the blog is not finished, and I'm not an expert on everything I've done so far. This article is more of a journal on some of the things I've done so far. Some great resources here. Check out Nafiul Islam's pelican setup guide . Start-up Download Python which should include pip which is a package manager. Using pip, install Pelican. Use the quick set up, I only chose to say Yes to uploading it to a Github page one day. Once I had a test site I started looking for a theme. It's fun to learn by picking things apart, a lot better than digging around in docs and failing a million ways. Themes Pelican has a repository of default themes you can choose. They recommend downloading the whole thing which I did but now seems rather unnecessary. I started by using foundation-default-colours. I poked around in the theme but then found features in another blog I wanted to work with. So off to 'Plumage' I went. It's not a default theme but it was one I wanted to work with. Customization It took a few days to start getting something clean but customized. I changed the backgrounds to some Japanese inspired patterns that look subtle and organized to me. Changed the Pygment to the solarized light instead of the solarized dark, dark code background was too jarring. Then I found tango is a nice default Pygment so I compiled it to a css and inserted it. The theme was coded to option allow a Disqus comment section but after some research and just plain preference I decided to use a static comment system . This is a rather weird setup which won't scale well, but it's very nice for what I want now. No account needed to comment, full markdown support for comments, all personal information and data handling is performed by me not an independent service. The downside is comments aren't immediately posted and the stock handling is I have to review and add the comment manually. I may make this automatic but handling comments manually is appealing right now. Added a Show/Hide toggler courtesy of flavio . He combines it with the pelican plug-in pelican_gist which offers a simple Markdown tactic to insert blocks of text that can easily be hidden by the reader. Concise in the article write up, concise on the article page. Added pelican_gist which pulls up stored code from my gist. At this point the blog looks good enough to put up. I'm interested in continuing to push pelican/markdown some more so I'll keep poking around.","tags":"Projects","url":"my-first-blog-ill-use-pelican.html"},{"title":"How to install an openSUSE distro with dual boot","text":"I pulled out my old desktop and decided to put a linux distro on it so I could learn some popular database operating systems. Not sure I want to go into how to select one here, too many articles on it already. According to my research some good personal distros seem to be Ubuntu or Fedora, and some more commercial oriented ones (still free) are CentOS or openSUSE. A good paid one appears to be Redhat, CentOS is the freeware version (no support included). I decided to go with openSUSE, but I will likely be working with CentOS as well. Dual boot openSUSE This desktop was one I built and had installed Win7 with an eventual Win10 upgrade. The goal therefore was to install openSUSE alongside my existing Windows 10. To make a long story short, this turned out to be impossible as my main drive was MBR formatted. What I needed to dual boot easily was a GPT formatted drive. Here is an article discussing the differences. Now there are some unofficial utilities that claim to migrate files in a reformat safely. But the confidence wasn't there so I decided to do a rescue operation of the C drive, take out everything I wanted and just reformat entirely to GPT. I have a laptop that I used to write a openSUSE distro in USB boot drive. I chose Leap as it is the stable released version they offer. They also offer Tumbleweed which is the more experimental rolling version featuring the latest, also claimed to be stable. Installation I wrote to the usb using Rufus which specializes in creating bootable USB drives. Need to find your Windows key? Check out this VB script courtesy of thewindowsclub. Here is the code itself. Open a text file and paste in this code and save as a .vbs file. Then just click the file to generate it. I should note here are some simpler methods. Show/hide Windows key finder vbs script Set WshShell = CreateObject(\"WScript.Shell\") MsgBox ConvertToKey(WshShell.RegRead(\"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\DigitalProductId\")) Function ConvertToKey(Key) Const KeyOffset = 52 i = 28 Chars = \"BCDFGHJKMPQRTVWXY2346789\" Do Cur = 0 x = 14 Do Cur = Cur * 256 Cur = Key(x + KeyOffset) + Cur Key(x + KeyOffset) = (Cur \\ 24) And 255 Cur = Cur Mod 24 x = x -1 Loop While x >= 0 i = i -1 KeyOutput = Mid(Chars, Cur + 1, 1) & KeyOutput If (((29 - i) Mod 6) = 0) And (i <> -1) Then i = i -1 KeyOutput = \"-\" & KeyOutput End If Loop While i >= 0 ConvertToKey = KeyOutput End Function Once you have a copy of Windows that corresponds to what you are licensed to use, you might want to go ahead and find your windows key for after the installation when you need to reactivate. If you're simply wiping your drive and reinstalling then Windows will very likely automatically activate your copy. They monitor your hardware signature and will approve it if nothing has changed drastically. You may expect some hassle if some parts change and have to speak to customer service if a lot has changed. I don't know the entire policy since thankfully I didn't need to use my product key as it was automatically activated. But you should follow earlier directions and store your Windows key just in case. But wait, having trouble getting a Windows copy? Windows really doesn't want to serve their OS for people, I imagine the server costs would be unfathomable. So almost no matter how botched your copy is or what weird situation you're in, Windows doesn't want to give you a copy, they just keep improving their reinstall software. There is a solution, check out Windows ISO Downloader from heidoc It worked great for me. Armed with a windows key and a copy of Windows, I pulled the trigger and reformatted my C drive completely. (I am working on a SSD here for anyone curious.) Ok now that you have your two USB boot drives, one with Windows, one with your linux distro. (You could also just use the same one, but two is simpler). Install Windows first, openSUSE does a decent job of detecting an existing OS and Windows is territorial. Alright you've installed Windows. That was the easy part. Now put in your linux USB and boot from it. Since you went through the trouble of ensuring a GPT format, this should be rather easy. Choosing the recommended Btrfs file format will create an almost countless assortment of partitions. Sticking with ntfs cuts it down to 3 partitions. The option you chose when creating your USB with Rufus should've specified UEFI for the boot drive, it's the default. Once you finish the installation prompts for both operating systems you will face an option upon booting up to choose which installed boot drive to access. My openSUSE is the default which starts after a countdown. Well that was it, wasn't too bad. I use KDE which is a nice gui or \"desktop environment\" and the recommended one for openSUSE, haven't found an error yet.","tags":"Hardware","url":"how-to-install-an-opensuse-distro-with-dual-boot.html"}]}